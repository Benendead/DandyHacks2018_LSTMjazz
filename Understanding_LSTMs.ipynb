{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Understanding LSTMs",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "zxaNbKNijLyg",
        "n9HMLzwelVP1",
        "CFJYCp6Yox2f",
        "4oompjF8u_53",
        "3TdT0A9VxMjl",
        "ClGMb1RQm3UJ",
        "wfWNAwGVwsIc"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benendead/LSTMjazz/blob/master/Understanding_LSTMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wjH5HyuDB8GM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Source 1 : [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
      ]
    },
    {
      "metadata": {
        "id": "zxaNbKNijLyg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks (RNNs)"
      ]
    },
    {
      "metadata": {
        "id": "Y8DZis-JjRgf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Traditional ANNs are unable to perceive the sequential context of data points, which is of course a fundamental ability in comprehending all sorts of things."
      ]
    },
    {
      "metadata": {
        "id": "1Bi0Q_lQjnX0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "RNNs address this by including loops where data can persist. For input $x_t$, the network outputs some $h_t$. On the next input, $x_{t+1}$, the RNN receives persisting data from earlier inputs."
      ]
    },
    {
      "metadata": {
        "id": "rNYgj2Fikrva",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![RNN unrolled](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)"
      ]
    },
    {
      "metadata": {
        "id": "ZDr1K-GxkzOe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "RNNs, with this sequential architecture, have been successfully applied to numerous problems including speech recognition, translation, and image captioning. That said, LSTMs have been crucial for these successes."
      ]
    },
    {
      "metadata": {
        "id": "n9HMLzwelVP1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RNNs: Bad at Long-Term Dependencies"
      ]
    },
    {
      "metadata": {
        "id": "shxsMUmKljWU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Based on what we've seen, RNNs should be able to connect previously seen information to their present task. As an example, say the network needs to predict the next word in a sentence based on previous words. If it's given \"the clouds are in the ____,\" it's pretty obvious that the next word will be \"sky.\" The immediate context enables the RNN to use the past information."
      ]
    },
    {
      "metadata": {
        "id": "6eNnz5OTmhLN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Consider a much longer paragraph beginning, \"I grew up in France...\" and ending with \"I speak fluent ____.\" Recent context might inform the RNN that the next word will be a language. Unfortunately, the context of France is a much more distant memory."
      ]
    },
    {
      "metadata": {
        "id": "eTPNxOxQnUj9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As this gap grows, RNNs are unable to connect the distant information. In theory, they're capable of handling these \"long-term dependencies,\" but in practice they're unable to learn the exact parameters to do so on their own. To quote the title of Yoshua Bengio's [1994 paper](http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf) on the topic, \"learning long-term dependencies with gradient descent is difficult.\""
      ]
    },
    {
      "metadata": {
        "id": "CFJYCp6Yox2f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## LSTMs: Good at Long-Term Dependencies"
      ]
    },
    {
      "metadata": {
        "id": "z2Fo5l0zo_0K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Long Short Term Memory networks - shortened to \"LSTMs\" - are a special kind of RNN capable of long-term dependencies. They were introduced in 1997 in [this paper](http://www.bioinf.jku.at/publications/older/2604.pdf). LSTMs were designed to solve the long-term dependency problem, and thus remembering information for a long time is basically their default behavior."
      ]
    },
    {
      "metadata": {
        "id": "Kzw_OlG9p1sZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Consider the structure of a typical RNN:"
      ]
    },
    {
      "metadata": {
        "id": "MAbY-vDQp67o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![RNN Chain](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)"
      ]
    },
    {
      "metadata": {
        "id": "uMf66vPbqOs5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the RNN, the chain of repeating modules is quite simple: the output of each layer is combined with the next input. We then $\\text{tanh}$ the whole thing.\n",
        "\n",
        "For clarity, consider another RNN illustration:"
      ]
    },
    {
      "metadata": {
        "id": "Kx0OFOa3rOJv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![RNN Alternative View](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "_AbbuhXJrpks",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notation from [another source](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/):\n",
        "* $x_t$ is input at time $t$.\n",
        "* $s_t$ is the hidden state at time $t$. It's the memory of the RNN and is calculated as $s_t=f(Ux_t+Ws_{t+1})$. $f$ is a nonlinearity like $\\text{tanh}$ or $\\text{ReLU}$.\n",
        "* $o_t$ is the output at time $t$. As an example, it might be $\\text{softmax}(Vs_t)$ if we wanted to select the highest probabilty of the options in the output vector.\n",
        "* $U$, $V$, and $W$ are the same for each module, as can be seen on the left."
      ]
    },
    {
      "metadata": {
        "id": "hLSbTo_Yp7D9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now consider the structure of an LSTM:"
      ]
    },
    {
      "metadata": {
        "id": "dOOU11K2qHSR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![LSTM Chain](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)"
      ]
    },
    {
      "metadata": {
        "id": "DzUmIC-AuVlf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LSTMs instead have four neural network layers per module. The notation used is as follows:"
      ]
    },
    {
      "metadata": {
        "id": "6lLhda45uuK2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![RNN Notation](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png)"
      ]
    },
    {
      "metadata": {
        "id": "OR_iHXwIuyzS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll go into the specifics of the LSTM structure later."
      ]
    },
    {
      "metadata": {
        "id": "4oompjF8u_53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Core Idea of LSTMs"
      ]
    },
    {
      "metadata": {
        "id": "-iTgUiwmvDlo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The centerpiece of the LSTM is the cell state, seen as the top line in each module. These run straight down the chain, with optional changes occasionally altering the information passed along. The few ways the LSTM can add or remove data from the cell state are called gates."
      ]
    },
    {
      "metadata": {
        "id": "5kK_8661vo2I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Gates optionally let information through. They're composed of a sigmoid layer and a pointwise multiplication operation. Sigmoid (by definition) outputs numbers zero to one, where a zero says \"change nothing in the cell state\" and a one meaning \"let everything through!\""
      ]
    },
    {
      "metadata": {
        "id": "9qgB5R5mwPQH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LSTMs have three of these gates:\n",
        "1. Forget gate\n",
        "2. Input gate\n",
        "3. Output gate"
      ]
    },
    {
      "metadata": {
        "id": "3TdT0A9VxMjl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step-by-step Walkthrough"
      ]
    },
    {
      "metadata": {
        "id": "N9-kMFbixQEE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 1: Decide what information in the cell state should be thrown away. This is made by the \"forget gate layer.\" Looks at $h_{t-1}$ and $x_t$ and outputs a number 0-1 for each number in cell state $C_{t-1}$. 1 says to keep and 0 says to completely remove.  \n",
        "![Forget Gate Graphic](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)"
      ]
    },
    {
      "metadata": {
        "id": "1VgMCsJBiucT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 2: Decide what information we should store in the cell state. This has two subprocesses: a $\\text{sigmoid}$ layer called the \"input gate layer\" decides which values to update, and a $\\text{tanh}$ layer creates a vector of candidate values, $\\tilde{C}_t$,  as potential options to add to the cell state.  \n",
        "![Input Gate Layer Graphic](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)"
      ]
    },
    {
      "metadata": {
        "id": "715erDvpkHkR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 3: Update the old cell state $C_{t-1}$ into new cell state $C_t$. We already have *what* to do based on the previous steps, but we now *do* it:  \n",
        "1. Multiply $C_{t-1}$ by $f_t$, the forget gate's output. This \"forgets\" what we no longer need.\n",
        "2. Add $i_t*\\tilde{C}_t$ to this result. The new candidate values, scaled by their importance, were what we'd decided was worth remembering from this step.  \n",
        "![Update Cell State Graphic](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)"
      ]
    },
    {
      "metadata": {
        "id": "jV605YPsi3pR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 4: Decide what we'd like to output. This will be a filtered version of our cell state:\n",
        "1. We use a sigmoid layer over to determine which parts of the cell state to output.\n",
        "2. We put the cell state through a $\\text{tanh}$ layer to push the values between -1 and 1.\n",
        "3. Multiply the sigmoid's result by the tanh scaling as to output only what we decided on.  \n",
        "![Output Layers Graphic](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)"
      ]
    },
    {
      "metadata": {
        "id": "ClGMb1RQm3UJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Variants on LSTMs"
      ]
    },
    {
      "metadata": {
        "id": "V1F7OXh0m5Ai",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We've thus far described \"a pretty normal LSTM,\" but in reality almost every paper using LSTMs uses a slightly different version."
      ]
    },
    {
      "metadata": {
        "id": "a9c41X_wnhUs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Peephole Connections** - Introduced in 2000 by [Gers & Schmidhuber](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf). These additional connections allow the gate layers to look at the cell state. Many papers only give some gates peepholes and not others.  \n",
        "![Peephole Connections Graphic](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png)"
      ]
    },
    {
      "metadata": {
        "id": "MeoXeVI_pSDd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Coupled Forget and Input Gates** - Instead of separately deciding where to forget or add new information, we combine those decisions. We only forget something if we're going to input something in its place.  \n",
        "![Coupled Forget/Input Gates Graphic](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png)"
      ]
    },
    {
      "metadata": {
        "id": "tM8rHFuor_i6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**GRU** - Gated Recurrent Units, or GRUs, were introduced by [Cho, et al.](https://arxiv.org/pdf/1406.1078v3.pdf) in 2014. These modules combine the forget and input gates into a single \"update gate.\" They also merge the cell and hidden states, among other changes. The result is simpler than standard LSTMs and was growing in popularity as of 2015.  \n",
        "![GRU Graphic](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png)"
      ]
    },
    {
      "metadata": {
        "id": "7D4jq0P_ulkp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Additional Variants** - There are numerous other LSTM variants. Some of these include [Depth Gated RNNs](https://arxiv.org/pdf/1508.03790v2.pdf) or [Clockwork RNNs](https://arxiv.org/pdf/1402.3511v1.pdf). In general, most variants achieve pretty similar results, but of course [a few can do better than LSTMs](http://proceedings.mlr.press/v37/jozefowicz15.pdf) on certain tasks."
      ]
    },
    {
      "metadata": {
        "id": "E3nCLQKeyFBL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "wfWNAwGVwsIc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ]
    },
    {
      "metadata": {
        "id": "F4Qdm57hxGyb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LSTMs are responsible for many of the achievements being made with RNNs. The model may be complex, but hopefully it's a bit more approachable now. The next step, in many researchers' opinions, is the use of attention in ML/DL models."
      ]
    },
    {
      "metadata": {
        "id": "oIaTzn5kmrNb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Future Things To Read"
      ]
    },
    {
      "metadata": {
        "id": "6u6U1QZPxTIC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On this: https://skymind.ai/wiki/lstm\n",
        "\n",
        "Future:\n",
        "https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714\n",
        "https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47\n",
        "https://towardsdatascience.com/recurrent-neural-networks-and-lstm-4b601dd822a5\n",
        "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21"
      ]
    }
  ]
}