{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Preprocessing",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "4MIWGuuTyDdh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benned-H/LSTMjazz/blob/master/Data_Processing/Final_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AMvZWMG5dyK",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "Author(s) | Year | Models Used | Music | Encoding | Quantization | Future work | Code/Examples\n",
        "--- | --- | --- | --- | --- | --- | --- | ---\n",
        "Eck | 2002 | LSTM | Melody + chords | 13 melody, 12 chord 1/0 | 2 per beat | N/A | [Ex](https://web.archive.org/web/20190104192500/http://people.idsia.ch/~juergen/blues/)\n",
        "Bickerman | 2010 | DBN | Chords -> jazz licks | 18 melody (12 pitch, 4 8va), 12 chord | 12 per beat | Melodies avoid triplets | [Code](https://sourceforge.net/projects/rbm-provisor/)\n",
        "Choi | 2016 | char-RNN, word-RNN | Jazz chord progressions | Note chars, Chord words | 1 per beat | N/A | [Code](https://github.com/keunwoochoi/lstm_real_book)\n",
        "Lackner | 2016 | LSTM | Melody given chords | 24 melody, 12 chord 1/0 | 4 per beat | Larger dataset | [Ex](https://konstilackner.github.io/LSTM-RNN-Melody-Composer-Website/)\n",
        "Agarwala | 2017 | Seq2Seq, char-RNN | Melodies | ABC char -> embeddings | None; ABC notation | N/A | [Code](https://github.com/yinoue93/CS224N_proj)\n",
        "Brunner | 2017 | 2 LSTMs | Chords -> polyphonic piano | 48 melody, 50 chord embeddings | 2 per beat | Encoding polyphonic sustain, genre metadata | N/A\n",
        "Hilscher | 2018 | char-RNN | Polyphonic piano | 1/0 on/off vectors | 4 per beat | More keys/data, text pattern matching | [Ex](https://yellow-ray.de/~moritz/midi_rnn/examples.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VxmZvUCTmc_",
        "colab_type": "code",
        "outputId": "a1697939-f67e-455f-af2f-9de8891d01a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFlwt2tyUTsb",
        "colab_type": "code",
        "outputId": "0cdf8820-e1a3-437d-ad47-5f943e60e2f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1YoQIYVTqar",
        "colab_type": "code",
        "outputId": "57833754-9b28-43db-c35c-eb1340aae6b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Navigate to the Datasets folder.\n",
        "os.chdir('gdrive/My Drive/Datasets')\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Chords Bits'  'Chords Tokens'\t'Melodies 18-bit'  'Melodies Piano Roll'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFY2-lBNUfiu",
        "colab_type": "code",
        "outputId": "07c4ab5a-9cce-4fd6-e560-28d8e968ef62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# We have all of our data in these subfolders.\n",
        "len(glob.glob(\"*/*.csv\"))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MIWGuuTyDdh",
        "colab_type": "text"
      },
      "source": [
        "# Additional Data Formatting Considerations  \n",
        "IGNORE FOR NOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Lh6E9d1Mq8",
        "colab_type": "text"
      },
      "source": [
        "In the work of [Brunner et al.](https://arxiv.org/abs/1711.07682) (2017), their LSTM received vectors of piano rolls with these appended features:\n",
        "1. Embedded chord vector of the next time step. (Current chord)\n",
        "2. Embedded chord vector of the chord following that chord.\n",
        "3. A binary counter from 0 to 7 each bar.\n",
        "\n",
        "Because my timesteps are much finer, I'd alter these features to the following:\n",
        "1. \"Current\" embedded chord vector for this timestep.\n",
        "2. Embedded chord vector of the chord following that chord.\n",
        "3. See below for different timing bit vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydoa7p-R1poJ",
        "colab_type": "text"
      },
      "source": [
        "In my case, I would need to count to 48 OR simplify the bits a bit to give information only about on-beat, off-beat, sixteenth, and triplet information. What about:\n",
        "\n",
        "---\n",
        "Bit 8 | Bit 7 | Bit 6 | Bit 5 | Bit 4 | Bit 3 | Bit 2 | Bit 1\n",
        "--- | ---\n",
        "Third triplet | Second triplet | Any triplet | On any down-beat | On any half-note | On-beat | On any 8th | On any 16th\n",
        "Offset % 12 = 8 | Offset % 12 = 4 | Offset % 4 = 0 | Offset % 48 = 0 | Offset % 24 = 0 | Offset % 12 = 0 | Offset % 6 = 0  | Offset % 3 = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK-UYTQO24QT",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "Bit 7 | Bit 6 | Bit 5 | Bit 4 | Bit 3 | Bit 2 | Bit 1\n",
        "--- | ---\n",
        "Third triplet | Second triplet | On-beat | On any 8th | On any 16th | Count 4 beats Bit 2 | Count 4 beats Bit 1\n",
        "Offset % 12 = 8 | Offset % 12 = 4 | Offset % 12 = 0 | Offset % 6 = 0 | Offset % 3 = 0 | ???  | ???\n",
        "\n",
        "To be clear, these features give the LSTM an easier job knowing when to change notes. To that end, I'd argue that such assistance might normally come from a rhythm section, and it's not inherently \"unfair\" information to give the system. What it might cause is a much clearer bias towards on-beat or on-eighth (etc.) notes, but balancing temperature and over-fitting can hopefully alleviate these concerns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTf6At9t7WG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def binaryCounter(offset):\n",
        "  \"\"\"\n",
        "  Returns a DataFrame based on the above encodings given an offset.\n",
        "  \"\"\"\n",
        "  #bit8 = (offset % 12 == 8)\n",
        "  #bit7 = (offset % 12 == 4)\n",
        "  #bit6 = (offset % 4 == 0)\n",
        "  #bit5 = (offset % 48 == 0)\n",
        "  #bit4 = (offset % 24 == 0)\n",
        "  #bit3 = (offset % 12 == 0)\n",
        "  #bit2 = (offset % 6 == 0)\n",
        "  #bit1 = (offset % 3 == 0)\n",
        "  #return pd.DataFrame(np.array([bit8, bit7, bit6, bit5, bit4, bit3, bit2, bit1])).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNcH3hljVt29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "binaryCounter(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFXGKnQuFzrw",
        "colab_type": "text"
      },
      "source": [
        "**NOTE: THE ABOVE CODE IS INCOMPLETE UNTIL I'VE TESTED WITHOUT THESE FEATURES FIRST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dj1sjHY9H18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "os.chdir(\"Melodies Piano Roll\")\n",
        "!ls\n",
        "\n",
        "c_bits = glob.glob(\"*.csv\")\n",
        "#c_tokens = glob.glob(\"*.csv\")\n",
        "#m_18bits = glob.glob(\"*.csv\")\n",
        "#m_piano = glob.glob(\"*.csv\")\n",
        "\n",
        "names = []\n",
        "for c in c_bits:\n",
        "  names.append(c[:-4])\n",
        "name_indices = dict((n, i) for i, n in enumerate(names))\n",
        "\n",
        "for i, c in enumerate(m_piano):\n",
        "  df = pd.read_csv(c)\n",
        "  new_name = str(name_indices[c[:-4]]) + \".csv\"\n",
        "  df.to_csv(new_name)\n",
        "  print(i)\n",
        "  \n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O46QH-7F3WG",
        "colab_type": "text"
      },
      "source": [
        "The above code was used to rename all csv so they're more clearly compatible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW81R5ezF7O5",
        "colab_type": "text"
      },
      "source": [
        "# Sampling The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFNbUI1MyP5H",
        "colab_type": "text"
      },
      "source": [
        "Per my past summaries of previous works, we have this process for formatting data for Keras:\n",
        "\n",
        "We have dataset $D=(X, Y)$ of \"labelled\" chord progression segments: $X = \\{X_1, X_2, ..., X_n\\}$ and $Y = \\{Y_1, Y_2, ..., Y_n\\}$, where each $X_i$ is some section of chord progression and each $Y_i$ is the corresponding melody label. My original piano matrix is of dimensions $(\\text{# timesteps}, |\\text{note range}|)$. In the 18-bit case, this is more simply $(\\text{# timesteps}, 18)$.\n",
        "\n",
        "First, we need to sample these matrices into $t$-timestep-long sequences of chord data (these are our $X_i$). We'll then label each of these with the melody information from the $t+1$ timestep. The number of samples, $S$, will be the total length of each song (in timesteps) minus $(t+1)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDXhlawYGKkh",
        "colab_type": "text"
      },
      "source": [
        "The final data shape should be a 3D matrix of dimensions $(\\text{# samples}, \\text{time steps}, \\text{features})$. Therefore we'll just need to sample the piano and 18-bit matrices into samples without any other dimensional shifting. But *what sample sizes did my sources use?*\n",
        "* Choi - Sampled 20 characters at a time with step size 3.\n",
        "* Lackner - Seems to be 8, or 2 beats worth.\n",
        "* Hilscher - Sentence length 100 (used chars)\n",
        "\n",
        "The rest of my sources didn't really say."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W4fUUmD7nq1",
        "colab_type": "text"
      },
      "source": [
        "**Conclusion** - I think two bars would be a good place to start. These would be $(8\\text{ beats}*12\\text{ ticks/beat}) = 96$ time steps. I'd also consider sampling at an interval of 5, as an example, to decrease redundancy if training takes too long. Until I hit this wall, I'll use a step size between windows of 1. So let's get to sampling, then.\n",
        "\n",
        "Edit: Lackner uses 8-bar samples overall in his code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKWBqo2T7rm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(X_df, Y_df, sample_length, step):\n",
        "  \"\"\"\n",
        "  Splits the given dataframes into overlapping samples each beginning the given step size apart.\n",
        "  X output will be the sliding windows from X\n",
        "  Y output will be single-row labels from the Y DataFrame.\n",
        "  \n",
        "  df : The Pandas DataFrame to be sliced into samples.\n",
        "  sample_length : int\n",
        "    The desired length of each sample.\n",
        "  step : int\n",
        "    The number of timesteps between where each sample begins.\n",
        "  \"\"\"\n",
        "  if (not len(X_df) == len(Y_df)):\n",
        "    print(\"These dataframes have different length!\")\n",
        "    return 0,0\n",
        "  \n",
        "  length = X_df.shape[0]\n",
        "  Xsamples = []\n",
        "  Ysamples = []\n",
        "  \n",
        "  for i in range(0, (length - sample_length - 1), step):\n",
        "    Xsample = X_df.iloc[i:i+sample_length, :].values\n",
        "    Ysample = Y_df.iloc[i+sample_length, :].values\n",
        "    \n",
        "    Xsamples.append(Xsample)\n",
        "    Ysamples.append(Ysample)\n",
        "    \n",
        "  return Xsamples, Ysamples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-coFnTiN87d1",
        "colab_type": "text"
      },
      "source": [
        "# Reading in the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vki1kWjjy6iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files_to_read = 36 # Eventually 600\n",
        "sample_len = 24\n",
        "step_size = 1\n",
        "\n",
        "\n",
        "ChB = []\n",
        "ChT = []\n",
        "M18 = []\n",
        "MPR = []\n",
        "\n",
        "# Running the below block, in order, will import the above number of files from each of the four data encodings.\n",
        "# Begin in the Datasets directory."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaWn_O8VzFul",
        "colab_type": "text"
      },
      "source": [
        "## Run this section to import all 4 data encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl-a1gtrzXNT",
        "colab_type": "code",
        "outputId": "b3c17755-5b4f-4495-85dc-fac3c8c4e094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Chords Bits\n",
        "\n",
        "# Move into directory\n",
        "os.chdir(\"Chords Bits\")\n",
        "\n",
        "# Import data\n",
        "for i in range(files_to_read):\n",
        "  filename = str(i) + \".csv\"\n",
        "  df = pd.read_csv(filename)\n",
        "  ChB.append(df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])) # Drop waste columns from previous exports\n",
        "  if (i % 10 == 0):\n",
        "    clear_output()\n",
        "    print(\"Read ChB file\", i)\n",
        "\n",
        "# Move back into Datasets directory.\n",
        "os.chdir(\"..\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read ChB file 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaHBPCi50Twb",
        "colab_type": "code",
        "outputId": "0625ba9f-6f4d-446b-cd8e-14ae3f3cb91a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Chords Tokens\n",
        "\n",
        "# Move into directory\n",
        "os.chdir(\"Chords Tokens\")\n",
        "\n",
        "# Import data\n",
        "for i in range(files_to_read):\n",
        "  filename = str(i) + \".csv\"\n",
        "  df = pd.read_csv(filename)\n",
        "  ChT.append(df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])) # Drop waste columns from previous exports\n",
        "  if (i % 10 == 0):\n",
        "    clear_output()\n",
        "    print(\"Read ChT file\", i)\n",
        "\n",
        "# Move back into Datasets directory.\n",
        "os.chdir(\"..\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read ChT file 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX6eykfK0UU7",
        "colab_type": "code",
        "outputId": "4832aa61-119f-457b-d584-f103f69c9232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Melodies 18-bit\n",
        "\n",
        "# Move into directory\n",
        "os.chdir(\"Melodies 18-bit\")\n",
        "\n",
        "# Import data\n",
        "for i in range(files_to_read):\n",
        "  filename = str(i) + \".csv\"\n",
        "  df = pd.read_csv(filename)\n",
        "  M18.append(df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])) # Drop waste columns from previous exports\n",
        "  if (i % 10 == 0):\n",
        "    clear_output()\n",
        "    print(\"Read M18 file\", i)\n",
        "\n",
        "# Move back into Datasets directory.\n",
        "os.chdir(\"..\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read M18 file 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46wL7zSJ0Uyh",
        "colab_type": "code",
        "outputId": "52aa2b1d-5e3f-445e-a821-cc035e775945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Melodies Piano Roll\n",
        "\n",
        "# Move into directory\n",
        "os.chdir(\"Melodies Piano Roll\")\n",
        "\n",
        "# Import data\n",
        "for i in range(files_to_read):\n",
        "  filename = str(i) + \".csv\"\n",
        "  df = pd.read_csv(filename)\n",
        "  MPR.append(df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])) # Drop waste columns from previous exports\n",
        "  if (i % 10 == 0):\n",
        "    clear_output()\n",
        "    print(\"Read MPR file\", i)\n",
        "\n",
        "# Move back into Datasets directory.\n",
        "os.chdir(\"..\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read MPR file 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJIM3yLv00zm",
        "colab_type": "code",
        "outputId": "cd9b5163-fe75-4e5a-db4d-e8eb579dc7f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Verify\n",
        "print(len(ChB), len(ChT), len(M18), len(MPR))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36 36 36 36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urNR5DKW1pDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verify corresponding dfs are same length\n",
        "for i in range(files_to_read):\n",
        "  len1 = len(ChB[i])\n",
        "  len2 = len(ChT[i])\n",
        "  len3 = len(M18[i])\n",
        "  len4 = len(MPR[i])\n",
        "  \n",
        "  if (not (len1 == len2 and len1 == len3 and len1 == len4 and len2 == len3 and len3 == len4 and len2 == len3)):\n",
        "    printf(\"Dataframe\", i, \"was a problem:\", len1, len2, len3, len4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppNja0XMBf5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6ce672b7-b86e-4f86-980d-5b39fcaf1886"
      },
      "source": [
        "# Verify that some dataframes have different lengths\n",
        "print(\"Length of ChB[0]:\", len(ChB[0]))\n",
        "print(\"Length of ChB[12]:\", len(ChB[12]))\n",
        "print(\"Length of ChT[0]:\", len(ChT[0]))\n",
        "print(\"Length of ChT[12]:\", len(ChT[12]))\n",
        "print(\"Length of M18[0]:\", len(M18[0]))\n",
        "print(\"Length of M18[12]:\", len(M18[12]))\n",
        "print(\"Length of MPR[0]:\", len(MPR[0]))\n",
        "print(\"Length of MPR[12]:\", len(MPR[12]))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of ChB[0]: 3072\n",
            "Length of ChB[12]: 3120\n",
            "Length of ChT[0]: 3072\n",
            "Length of ChT[12]: 3120\n",
            "Length of M18[0]: 3072\n",
            "Length of M18[12]: 3120\n",
            "Length of MPR[0]: 3072\n",
            "Length of MPR[12]: 3120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Oclq620WMO",
        "colab_type": "text"
      },
      "source": [
        "## Sampling Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xfvyzGzxRvt",
        "colab_type": "code",
        "outputId": "31b204df-b770-41d8-df7c-e1683fcc87bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Let's try the ChB-M18 encoding pair.\n",
        "\n",
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "for i in range(files_to_read):\n",
        "  Xsam, Ysam = sample(ChB[i], M18[i], sample_len, step_size)\n",
        "  Xsam = np.array(Xsam)\n",
        "  Ysam = np.array(Ysam)\n",
        "  \n",
        "  X_train.append(Xsam)\n",
        "  Y_train.append(Ysam)\n",
        "  clear_output()\n",
        "  print(\"Sampling\", i + 1, \"of\", files_to_read)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sampling 36 of 36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJsHkbr0G1tl",
        "colab_type": "code",
        "outputId": "b425d1b2-f31f-440f-b830-bf9a587a95df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# What have we created?\n",
        "print(type(X_train), \"shape\", len(X_train))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> shape 36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05xnUq5WGYzO",
        "colab_type": "code",
        "outputId": "d2c446f2-950f-49d4-a566-2e7dc2b9f657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# A few songs' lengths.\n",
        "print(type(X_train[0]), \"shape\", X_train[0].shape)\n",
        "print(type(X_train[12]), \"shape\", X_train[12].shape)\n",
        "print(type(X_train[24]), \"shape\", X_train[24].shape)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> shape (3047, 24, 12)\n",
            "<class 'numpy.ndarray'> shape (3095, 24, 12)\n",
            "<class 'numpy.ndarray'> shape (3047, 24, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UpVFHOh6Prv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set our input and output widths based on our chosen encodings.\n",
        "input_width = 12\n",
        "output_width = 18"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHITyS9o6H6v",
        "colab_type": "text"
      },
      "source": [
        "# What structure should the network have?\n",
        "\n",
        "The sources I read had this to say (Taken from my summaries)  \n",
        "\n",
        "**Lackner (2016)**  \n",
        "The LSTM input layer took chord notes (thus 12 nodes), was fully connected to optional hidden layers, and then was fully connected to an output layer with 24 LSTM nodes corresponding to melody notes. The highest melody probability is chosen; if it's above some value, that note is played.\n",
        "\n",
        "To generate new music, any chord sequence can be input and the resulting output can be converted to MIDI. Many architectures were tested; the best had 2 hidden layers with 9 and 18 LSTM units, respectively.\n",
        "\n",
        "**Choi et al. (2016)**  \n",
        "Two LSTM layers with 512 hidden units (hidden state dimensionality) and then Dropout of 0.2 after each LSTM layer. Put together in Keras with categorical cross entropy as loss, Adam optimizer, and stochastic prediction based on a diversity parameter $\\alpha$. New probabilities are calculated:  \n",
        "$\\hat{p}_i=e^{log(p_i)/\\alpha}$, where $p_i$ is the probability for the $i$ states.  \n",
        "A state is then selected based on the probabilities.\n",
        "\n",
        "**Agarwala et al. (2017)**  \n",
        "Char-RNN: A hidden layer of 200 LSTM cells with 0.2 dropout and embedding size of 20 were chosen. Softmax was used to predict the next character. An input window of size 50 worked best.  \n",
        "\n",
        "**Brunner et al. (2017)**  \n",
        "Chord LSTM: This model learned those chord embeddings. From this layer, they used a hidden layer with 256 LSTM cells followed by a $\\text{softmax}$ activation. The output corresponded to a vector of probabilities for the next chord. Training used cross-entropy as loss, Adam optimizer, $10^{-5}$ initial learning rate, and 80,000 of the shifted songs for 4 epochs. To generate new progressions, they seed the model and then sample output probabilities with temperature. This is fed in and the cycle repeats.\n",
        "\n",
        "**Hilscher et al. (2018)**  \n",
        "The best architecture used a single LSTM layer of 512 units, sequence length of 100, Mozart data fully normalized/transposed, batch shuffling, with categorical cross entropy loss, Adam optimizer, 0.001 learning rate, and validation split of 0.2.\n",
        "\n",
        "Polyphonic LSTM: This LSTM received vectors of piano rolls (1/0 on/off made with the pretty_midi library) with these appended features:\n",
        "1. Embedded chord vector of the next time step.\n",
        "2. Embedded chord vector of the chord following that chord.\n",
        "3. A binary counter from 0 to 7 each bar.\n",
        "\n",
        "The input is fed into an LSTM with 512 hidden cells and $\\text{sigmoid}$ activation. The output at each time step is the probabilities for each note being played. Training used cross entropy between outputs and the ground truth, Adam, initial learning rate of $10^{-6}$, and only 10,000 songs for 4 epochs.\n",
        "\n",
        "To generate a new song, the polyphonic LSTM is seeded with the piano roll and corresponding chords. The next step is sampled and the number of notes to play at any one time is limited. To deal with ambiguous note endings/re-attacks, they consider all consecutive same notes as held notes. At barlines, all played notes are re-attacked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtdtoSVe7f3u",
        "colab_type": "text"
      },
      "source": [
        "Author(s) | Layers | Hidden State Size | Learning Rate | Loss | Optimizer\n",
        "--- | --- | --- | --- | --- | ---\n",
        "Lackner | Input (12), 9 LSTM units, 18 LSTM units, output 24 LSTM units | 9, 18, 24 | Unspecified | Unspecified | Unspecified\n",
        "Choi et al. | 512 LSTM units, Dropout 0.2, 512 LSTM units, Dropout 0.2 | 512 | Unspecified | Catgorical cross-entropy | ADAM\n",
        "Agarwala et al. | Embedding Size 20, 200 LSTM units w/ 0.2 Dropout | 200 | Unspecified | Unspecified | Unspecified\n",
        "Brunner et al. | Embedding, 256 LSTM units, softmax | 256 | 10^-5 | Cross-entropy | ADAM\n",
        "Hilscher et al. | Single LSTM layer of 512 units | 512 | 10^-6 | Cross-entropy | ADAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0FdEa_qAO5K",
        "colab_type": "text"
      },
      "source": [
        "To summarize, then, a good starting network for monophonic melody generation will probably have:\n",
        "* 1 to 3 LSTM layers\n",
        "* Maybe 200 units\n",
        "* Learning rate on the order of 10^-6\n",
        "* Cross-entropy loss is 100% the way to go\n",
        "* ADAM optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3bj5MPIAuIL",
        "colab_type": "text"
      },
      "source": [
        "# Build the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEdiBUfvBOL_",
        "colab_type": "text"
      },
      "source": [
        "Sources used to create this particular part of the project:\n",
        "\n",
        "1. https://keras.io/layers/recurrent/ Accessed 4/12/2019\n",
        "2. https://www.dlology.com/blog/how-to-use-return_state-or-return_sequences-in-keras/ Accessed 4/12/2019\n",
        "3. https://keras.io/getting-started/sequential-model-guide/ Accessed 5/22/2019"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXQ3e615DCWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM_units = [32, 64, 128] # Eventually, for now use simpler:\n",
        "LSTM_units = [8, 16, 32]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2MO-A_24DNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import statements\n",
        "from keras.layers import LSTM, Dropout, Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "input_dim = (sample_len, input_width)\n",
        "\n",
        "model = Sequential() # Declare our network.\n",
        "\n",
        "# Input layer\n",
        "model.add(LSTM(LSTM_units[0], return_sequences = True, input_shape = input_dim))\n",
        "\n",
        "# Add rest of the layers except last\n",
        "for i in range(1, len(LSTM_units) - 1):\n",
        "  model.add(LSTM(LSTM_units[i], return_sequences = True))\n",
        "  \n",
        "# Last LSTM layer doeesn't return a sequence.\n",
        "model.add(LSTM(LSTM_units[-1]))\n",
        "\n",
        "model.add(Dropout(0.5)) # How about a dropout layer? \n",
        "model.add(Dense(output_width, activation = 'sigmoid')) # Output layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7mYBKQ6y4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile our network\n",
        "model.compile(optimizer = 'adam',\n",
        "             loss = 'categorical_crossentropy',\n",
        "             metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFL2SCMeC1nq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, X_list, Y_list, epochs):\n",
        "  # Trains a given Keras model on the lists of training data samples.\n",
        "  if (len(X_list) != len(Y_list)):\n",
        "    print(\"Lists different length!\")\n",
        "    return\n",
        "  \n",
        "  num_songs = len(X_list)\n",
        "  for e in range(epochs):\n",
        "    # Train model on each song in training data.\n",
        "    # Batch size is length of one beat right now.\n",
        "    for i in range(num_songs):\n",
        "      clear_output()\n",
        "      print(\"Epoch\", e + 1, \"of\", str(epochs) + \". Training on song\", i + 1, \"of\", num_songs)\n",
        "      model.fit(X_list[i], Y_list[i], batch_size = 12, verbose = 1)\n",
        "\n",
        "# model.evaluate(X_list[i], Y_list[i], verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKg6TtvA83rp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fe7c3815-22f9-40e7-84c5-db23affd8604"
      },
      "source": [
        "train(model, X_train, Y_train, 10)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 7 of 10. Training on song 8 of 36\n",
            "Epoch 1/1\n",
            "1968/3047 [==================>...........] - ETA: 9s - loss: 1.7920 - acc: 0.6926Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYEZQpkxHY0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}