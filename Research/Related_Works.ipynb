{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Related Works",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Ad7hOxHDhuRM",
        "bUPSQfONkbv7",
        "Y0cgPgVxkZ0t",
        "HAtWbwdPt0YF",
        "nJ_vPUO1r80X",
        "-y-nj5aRr6Cn",
        "YhJIqhVRt-Gj",
        "ZFMxZYyKuUoX",
        "SwzQaOmCjXmQ",
        "EoSyAYPpxLzu",
        "qHtl1vB613eF",
        "L4wkueNNDehq",
        "Ft65OMY0Dh6U",
        "q0iwXod0J7xs",
        "VhZijWjhLrz9",
        "fXeuyK3wMGpY",
        "uKA5EpjAN10Y",
        "dD1CRIOtQ13Z",
        "_SAzrT4SV0K-",
        "feDYWEEgWiOx",
        "qJU9ZmPqSRhC",
        "Xiab-a4VVMf2",
        "nzezk5DAY5tk",
        "vXl_d0gNaTaG",
        "rzFiwA6Jd7i4",
        "AA3NVAoBcZky",
        "n__qnlEYivki",
        "vUzZHqdAkm2H",
        "bZBdiEiOpeXm",
        "nGyrc4bdpFLj",
        "5MuH4OSzqZJ3",
        "SQ4aTHCLrkkX",
        "I_QBEYVouCiz",
        "ThGV_a0EwCyO",
        "IJWg_omOY3Yi",
        "TsRGt7SL2yqI",
        "fua6n1U4583E",
        "Y5cWaBqdtP2_",
        "zuLBYfkOwPfv",
        "Ku700svMArj-",
        "Urld3i3SD-QD",
        "shkHh3bAEkN1",
        "GOF6022BpuHJ",
        "v7cD-bdmrtR-",
        "373jlgxPtAqK"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benendead/LSTMjazz/blob/master/Research/Related_Works.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Ad7hOxHDhuRM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Source 1: [Text-based LSTM networks for Automatic Music Composition](https://arxiv.org/pdf/1604.05358.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "bUPSQfONkbv7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ]
    },
    {
      "metadata": {
        "id": "q4eJDA7sijS3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This paper used text-based LSTMs for music composition. They found that word-RNNs could learn chord progressions and drum tracks, while char-RNNs could only succeed with chord progressions."
      ]
    },
    {
      "metadata": {
        "id": "PdlAFi-_jIa2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Their work attempted to generate jazz chord progressions and rock drum tracks. Their work is notable for two reasons:\n",
        "1. Their LSTMs were designed to learn from text data as opposed to music symbols or numeric values.\n",
        "2. They used a larger dataset than previous work, which allows learning of more complex chord progressions."
      ]
    },
    {
      "metadata": {
        "id": "Y0cgPgVxkZ0t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Architecture"
      ]
    },
    {
      "metadata": {
        "id": "mr-KvlVjkfXy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Char-RNNs are RNNs with character-based learning, as opposed to the conventional word-based learning. These return a vector corresponding to a character, while word-RNNs predict vectors corresponding to unique words. The merits of char-RNNs include:\n",
        "1. Minimal assumptions are made - there are no constraints on the form of the text representation. Can RNNs learn musical information with such a weak assumption?\n",
        "2. Fewer characters means fewer states, which reduces computational costs. Their chord vocabulary had 1,259 \"words\" yet this reduces to 39 characters."
      ]
    },
    {
      "metadata": {
        "id": "h52Z9E4el41S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It's worth noting that char-RNNs need to form longer-term dependencies, as the sequence becomes longer. LSTMs help with this, but the trade-off does have downsides we'll see later."
      ]
    },
    {
      "metadata": {
        "id": "kEHWzWTRmHPO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "They used two LSTM layers, each with 512 hidden units. According to [reddit](https://www.reddit.com/r/MachineLearning/comments/87djn7/d_what_is_meant_by_number_of_hidden_units_in_an/), this 512 refers to the dimensionality of the hidden state ([Keras documentation](https://keras.io/layers/recurrent/) supports this answer). Dropout of 0.2 was added after each LSTM layer."
      ]
    },
    {
      "metadata": {
        "id": "1nz7ikwQouud",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model was put together using *Keras*. Loss function was categorical cross-entropy and optimizer was ADAM. This performed just as well as SGD with Nestrov momentum."
      ]
    },
    {
      "metadata": {
        "id": "zWCe2ZtBpFhu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The prediction is stochastic in that each prediction for time $t$, the network outputs probabilities for every state. The system also had a diversity parameter $\\alpha$ in the prediction stage, which suppresses $(\\alpha < 1)$ or encourages $(\\alpha < 1)$ the diversity of prediction by re-weighting the probabilities. This is done using the formula:  \n",
        "$\\hat{p}_i=e^{log(p_i)/\\alpha}$, where $p_i$ is the probability for the $i$ states.  \n",
        "Finally, one of the states is selected using the re-weighted probabilities."
      ]
    },
    {
      "metadata": {
        "id": "TvsTC9haq-ep",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The same size and number of layers was kept for both the char- and word-RNN experiments. The [code](https://github.com/keunwoochoi/lstm_real_book) is available online. I'll look through it once I'm done with the paper."
      ]
    },
    {
      "metadata": {
        "id": "HAtWbwdPt0YF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Case Study: Chord Progressions"
      ]
    },
    {
      "metadata": {
        "id": "nJ_vPUO1r80X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Representation"
      ]
    },
    {
      "metadata": {
        "id": "tV764uN8t2kz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "They avoid vector representations of the chords and instead use only the text representations. They filled in each chord for all quarter notes during the chord and that's the dataset. Ex:  \n",
        "\n",
        "F:9 F:9 F:9 F:9 D:min7\n",
        "D:min7 G:9 G:9 C:maj C:maj\n",
        "F:9 F:9 C:maj C:maj C:maj\n",
        "C:maj"
      ]
    },
    {
      "metadata": {
        "id": "cLnnM3w6ufmD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Their dataset included 2,486 scores from real and fake books. They transposed everything to C and filled in the duration of chords per quarter note. All song data was appended using _START_ and _END_ flags."
      ]
    },
    {
      "metadata": {
        "id": "wABVyU99rBEw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Even when transposed to C, only 867 scores end in C:maj, 489 in G:7, 186 C:maj6, 52 F:maj, and 1,252 in other chords. There were 1,259 unique chords in the training data, which gave the word-RNN a vocab size of 1,259. There were 39 distinct characters, 539,609 chords, and 3,531,261 characters total."
      ]
    },
    {
      "metadata": {
        "id": "-y-nj5aRr6Cn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Results"
      ]
    },
    {
      "metadata": {
        "id": "GBdhXai5r74D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The system was set to output a chord progression for each diversity parameter after every iteration. Both the word- and char-RNNs showed well structured results. They learned local structures of chords and bars as well as the _START_ and _END_ tags."
      ]
    },
    {
      "metadata": {
        "id": "9K2-4CgmsoKI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After enough training, both results showed chord progressions from real jazz grammar. In  char-RNN, this included ii-V-I, passing chords, modal interchange chords, and substitutions (B:7 for F:7). For word-RNN, these included modal interchanges, circle of fifths (Eb:sus - Gb:maj6 - B:maj7), and descending bass."
      ]
    },
    {
      "metadata": {
        "id": "HTiwTQfDtfR5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The approaches differed slightly in output, as the word-RNN appeared to prefer more conventional progressions than in char-RNN. This may be caused by the different effective lengths of the approaches, as the char-RNN effective has a shorter memory span."
      ]
    },
    {
      "metadata": {
        "id": "YhJIqhVRt-Gj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ]
    },
    {
      "metadata": {
        "id": "DwtLYtmxt_eI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "They used text-based LSTMs to generate chord progressions based on jazz real books. Both word-RNNs and char-RNNs worked in this use case. Their usage of a diversity parameter gives composers a useful tool to dial in desired effects."
      ]
    },
    {
      "metadata": {
        "id": "ZFMxZYyKuUoX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Source 2: [Charlie Parker's Omnibook Data](https://members.loria.fr/KDeguernel/omnibook/)"
      ]
    },
    {
      "metadata": {
        "id": "RMju1urDvrSF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Just a note that this is the dataset we used. Citation below:"
      ]
    },
    {
      "metadata": {
        "id": "Ylo8ViINvu3H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using Multidimensional Sequences For Improvisation In The OMax Paradigm\n",
        "Ken Déguernel, Emmanuel Vincent, Gérard Assayag\n",
        "13th Sound and Music Computing Conference, Aug 2016, Hamburg, Germany. 〈http://quintetnet.hfmt-hamburg.de/SMC2016/〉"
      ]
    },
    {
      "metadata": {
        "id": "SwzQaOmCjXmQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Source 3: [A First Look at Music Composition using LSTM Recurrent Neural Networks](http://people.idsia.ch/~juergen/blues/IDSIA-07-02.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "hyCFd6Sgywbj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Abstract**  \n",
        "Music composed by RNNs typically suffers from a lack of global structure. Note-by-note transitions or phrase reproduction might succeed, but overall musical form eludes RNNs' grasp. Luckily, LSTMs can overcome the long-term dependency issues and create music that's surprisingly pleasing in the blues style."
      ]
    },
    {
      "metadata": {
        "id": "EoSyAYPpxLzu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "metadata": {
        "id": "QuqZbMePxNjM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A most basic attempt a composition with RNNs might consider single-step predictions. This can then be seeded with just one example and attempt to compose from there. To state the obvious, feed-forward networks coud never perform such a task, as they can't store information about the past. RNNs overcome this basic limitation, at least."
      ]
    },
    {
      "metadata": {
        "id": "hvU2mHFFzYKo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "RNNs still struggle with this task, unfortunately. Mozer, in 1994, wrote of his RNN's compositions that “While the\n",
        "local contours made sense, the pieces were not musically coherent, lacking thematic structure and having minimal phrase structure and rhythmic organization.\" This problem likely links to vanishing gradients, as in both BPTT and RTRL error flow tends to explode or vanish."
      ]
    },
    {
      "metadata": {
        "id": "_DqPXspYz-nm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Of course music requires this overall contour Mozer mentions. At the most basic form, early rock-and-roll might consist of four-bar phrases which easily become 32 events or more as the time step is defined as eighth notes. Before this paper, Mozer's single-note melodies were the most relevant work. Mozer had used RNNs with BPTT, probabilistic output values, and a psychologically realistic encoding that gave bias towards chromatically and harmonically related notes. He also encoded events in fewer time steps by treating all notes as one time step. Even still, the architecture failed to capture global musical structure."
      ]
    },
    {
      "metadata": {
        "id": "0B-lUonx3NMC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Mozer suggested that for a note-by-note method to work, it requires a network which can induce structure at multiple levels. This paper offers that network.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "qHtl1vB613eF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. An LSTM Music Composer"
      ]
    },
    {
      "metadata": {
        "id": "L4wkueNNDehq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### LSTM Overview"
      ]
    },
    {
      "metadata": {
        "id": "t-1qEAJq3kTH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To summarize LSTMs, they're designed to obtain constant error flow through time so that nothing explodes or vanishes. LSTMs use linear units called Constant Error Carousels (CECs), apparently. Really the CEC is just another way to represent the continuing cells we otherwise get when we consider time steps."
      ]
    },
    {
      "metadata": {
        "id": "9fu1cOCY5ETE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The gates which control information into the CEC include:\n",
        "1. Multiplicative **Input Gate** - Learns to protect the information passed into the CEC by rejecting irrelevant inputs.\n",
        "2. Multiplicative **Output Gate** - Learns to protect other areas from currently irrelevant memory contents of the CEC.\n",
        "3. **Forget Gate** - Learns to reset memory cells when their content is obsolete."
      ]
    },
    {
      "metadata": {
        "id": "EZvsFn1JC2yr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Learning is then done using a modified BPTT and a customized version of RTRL. That is, output units use BPTT, output gates use truncated BPTT, and the input and forget gates use truncated RTRL."
      ]
    },
    {
      "metadata": {
        "id": "Ft65OMY0Dh6U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Representation"
      ]
    },
    {
      "metadata": {
        "id": "f0chP2RzDjdl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This paper represented the data in simple local form, using one input per note, with 1 as on and 0 as off. In later experiments, they adjusted input units to have a mean of 0 and a standard deviation of 1 (???). This left it to the network to develop a bias towards chromatic or harmonic notes. Their reasons follow:\n",
        "1. Implicitly multivoice and makes no distinction between chords and melodies. (They implemented chords simply by including them in the single input vector)\n",
        "2. It's easy to generate probability distributions over the set of possible notes, as they can treat single notes as independent or dependent."
      ]
    },
    {
      "metadata": {
        "id": "iajG6uFiGyvj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Time was represented as a single input vector representing one slice of real time. The stepsize of each time step of course can vary; they chose an eighth note as theirs. This is preferable for LSTMs, as the network needs to learn relative durations of notes as to create rhythm and coutning."
      ]
    },
    {
      "metadata": {
        "id": "KeAujtV0HMBl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This representation does ignore two issues:\n",
        "1. There's no indication where a note ends. Thus eight eighth notes in a row of the same notes are equivalent to four quarters or a whole note. One way to overcome this could be to decrease the stepsize of quantization and mark note endings with a zero (???).\n",
        "2. A second method was suggested by [Todd in 1989](https://pdfs.semanticscholar.org/81e0/a57abde1bf2cc4b7cd772e0573e92069e8ef.pdf). This used special units in the network to indicate the beginning of notes. These authors were unsure how that might scale to multi-voice melodies."
      ]
    },
    {
      "metadata": {
        "id": "aSonx9vIH-oj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The LSTM was given a range of 12 notes for the chords and 13 notes for melodies. These were simply two sections of the vector but otherwise were represented no differently."
      ]
    },
    {
      "metadata": {
        "id": "q0iwXod0J7xs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training Data"
      ]
    },
    {
      "metadata": {
        "id": "V_YU6YCSJ-BJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This experiment used a form of 12-bar blues with a quantization step of 8 notes per bar. Each song was thus 96 time steps long. The same chords were used in every song, and they were inverted as to fit into the 12 note options for the chords. Experiment 1 used only the chords whereas Experiment 2 also included a single melody line based on the pentatonic scale."
      ]
    },
    {
      "metadata": {
        "id": "SbG8NHYTLg9X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training melodies were constructed by concatenating bar-long segments of music by the first author to fit each chord. The datasets were then constructed by concatenating random samples from the $(n=2^{12}=4096)$ possible songs. The melodies included no rests and were only quarter notes."
      ]
    },
    {
      "metadata": {
        "id": "VhZijWjhLrz9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Experiment 1 - Learning Chords"
      ]
    },
    {
      "metadata": {
        "id": "sYyLQ-xXLuY9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Can an LSTM reproduce a musical chord structure? The motivation here is to ensure that the LSTM doesn't require melody."
      ]
    },
    {
      "metadata": {
        "id": "fXeuyK3wMGpY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Network Topology and Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "srJGDC0vMKsX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The chords used were the same 12-bar blues. The network had four cell blocks containing 2 cells each, fully connected to each other and to the input layer. The output layer was fully connected to all cells and the input layer. Forget, input, and output gate biases for the four blocks were set to -0.5, -1.0, -1.5, and -2.0. This causes the blocks to come online one by one. Output biases were set to 0.5, the learning rate 0.00001, and momentum rate 0.9."
      ]
    },
    {
      "metadata": {
        "id": "P4OKAP57NA9Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Weights were burned after every timestep, as experiments showed that learning was faster if the network was reset after making one or a few gross errors. Resetting went as follows:  \n",
        "On error, burn existing weights, reset input pattern and clear partial derivatives, activations, and cell states. This was similar to Gers and Schmidhuber's (2000) approach. The output function was the logistic sigmoid with range [0,1]."
      ]
    },
    {
      "metadata": {
        "id": "uKA5EpjAN10Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training/Testing"
      ]
    },
    {
      "metadata": {
        "id": "-WtNn1ELN7PK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The goal was to predict the probability for a given note to be on or off. For predicting probabilities, RMSE is not appropriate and thus the network was trained using cross-entropy as the objective function. The error function $E_i$ for output activation $y_i$ and target $t_i$ was:  \n",
        "$E_i=-t_i*ln(y_i)-(1-t_i)*ln(1-y_i).$"
      ]
    },
    {
      "metadata": {
        "id": "tLosUKY0OyTm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This gives a $\\delta$ term at the output layer of $(t_i-y_i)$. See [Joost and Schiffmann](https://www.worldscientific.com/doi/abs/10.1142/S0218488598000100) (1998) for what exactly this means. Note that they treat the outputs as statistically inpedendent of one another. Even if this assumption is untrue, it allows the network to predict chords and melodies in parallel (as well as multi-voice melodies)."
      ]
    },
    {
      "metadata": {
        "id": "lD5fx5leQg-U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The testing started the network with the inputs from the first timestep and used network predictions for ensuing time steps. The decision threshold for chord notes was 0.5. Training ended once the network spit back the correct chord sequence in entirety."
      ]
    },
    {
      "metadata": {
        "id": "dD1CRIOtQ13Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Results"
      ]
    },
    {
      "metadata": {
        "id": "zhyoltRSQ34p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the end, the network could accomplish this task with a variety of learning rates and momentum rates. The network was able to generate continuing cycles of the progression as well. This result is not surprising, honestly. The learning time took anywhere from 15 to 45 minutes on a 1Ghz Pentium."
      ]
    },
    {
      "metadata": {
        "id": "_SAzrT4SV0K-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Experiment 2 - Learning Melody and Chords"
      ]
    },
    {
      "metadata": {
        "id": "YJPsC44bV36y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this experiment, both parts were learned. They continued learning until the chord structure was learned and cross-entropy was relatively low. Once learning ended, the network was seeded with a note or a series of notes and then allowed to compose freely. The goal was to test if LSTMs could learn chord/melody structure and then use that when composing new songs."
      ]
    },
    {
      "metadata": {
        "id": "feDYWEEgWiOx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Network Topology and Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "T7gNyWpaW_L4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this case, some cell blocks processed chord information and some melody information. Eight cell blocks with two cells each were used. Four blocks were fully connected to the inputs for melody. The chord cell blocks had recurrent connections to themselves and the melody blocks, whereas the melody blocks were only recurrently connected to other melody blocks. Another way to say this is that melody information never reaches chord cell blocks."
      ]
    },
    {
      "metadata": {
        "id": "AfjEXAMHRm_D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At the output layer, output units for chords are fully connected to chord blocks as well as chord inputs. The melody outputs were fully connected to melody blocks and melody inputs. This is discussed in Section 5."
      ]
    },
    {
      "metadata": {
        "id": "V2I80ZMlSEYW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Forget gate, input gate, and output gate biases were set to -0.5, -1.0, -1.5, and -2.0 for the chord and melody blocks. All other parameters were identical to Experiment 1."
      ]
    },
    {
      "metadata": {
        "id": "qJU9ZmPqSRhC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training/Testing"
      ]
    },
    {
      "metadata": {
        "id": "TEMsL8cUSTTW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The goal was to predict the probability a given note to be on or off. For chords, the same 0.5 decision threshold was used. The melody was restricted to choosing one note per time step. This is done by making the melody outputs sum to 1 and then using a uniform random number in [0,1] to pick the next note. Again, this is discussed in Section 5."
      ]
    },
    {
      "metadata": {
        "id": "Rtilrf2RSxie",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The network was trained until it learned the chord structure and objective error plateaued. The network was then used to compose music by providing a single note or series of notes and then presenting the network outputs as inputs to the next time step. No algorithmic or statistical method was used to evaluate the output music's quality."
      ]
    },
    {
      "metadata": {
        "id": "6vvCxzF8TQoC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Results"
      ]
    },
    {
      "metadata": {
        "id": "Xn8YuonwTR4e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The LSTM indeed composed blues music. It fully learned the chord structure and then used that to constrain its melody output. They urge the reader to visit [their site](https://people.idsia.ch/~juergen/blues/) (Just delete the s in https) to judge the LSTM's music themselves. The network's results do far better than randomly stumbling around the pentatonic scale. One limitation is the fundamental same chord progression, but the compositions are indeed remarkable nonetheless."
      ]
    },
    {
      "metadata": {
        "id": "Xiab-a4VVMf2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. Discussion"
      ]
    },
    {
      "metadata": {
        "id": "hsZJ-JHSVQME",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The experiments were clearly successful, and to the authors' knowledge, theirs was the first successful use of LSTMs to compose globally coherent music. They acknowledge that more research is needed to see whether the LSTM can deal with more difficult tasks."
      ]
    },
    {
      "metadata": {
        "id": "QmwxC7uBV9wC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training Data"
      ]
    },
    {
      "metadata": {
        "id": "VHTVuxXdWAAj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The chord structure, for one, was uniform across the training set. Thus this experiment might be more acccurately defined as learning to solo over a predefined form. Their time step was also quite low, as 8 notes per measure is quite easier than, say, 32 time steps per whole note. (My approach plans on 48 steps per whole note)"
      ]
    },
    {
      "metadata": {
        "id": "l4oikoESWnyl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Network Architecture"
      ]
    },
    {
      "metadata": {
        "id": "Tb4dvdm-WwWV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The network's connections were divided between chords and melody, with chords influencing melody but not vice-versa. This choice is justified by the fact that a soloist follows the chord structure supplied by a rhythm section. This choice does presume, though, that we know how to segment input chords from melodies. With jazz sheet music, though, changes are provided separately from melodies and so this is no huge problem. That said, classical music as well as audio signals mix the two."
      ]
    },
    {
      "metadata": {
        "id": "XntcYPdtXVuV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\"Much more research is warranted.\" Comparing BPTT and RTRL, as well as other RNN variants, would help support a claim about LSTM's relative effectiveness. A more interesting training set might also allow for more interesting compositions. Finally, research circa 2002 led the authors to believe that LSTM works better using a Kalman filter to update weights."
      ]
    },
    {
      "metadata": {
        "id": "cq5WMVDjX_42",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The current architecture is also limited to mere symbolic representations. If it were modified to work with MIDI or audio, it could be used for interactive improvisation. This would require dealing with temporal noise, but research from oscillator beat tracking models (Eck) to LSTM might help calm this noise."
      ]
    },
    {
      "metadata": {
        "id": "UlRNkFe2YUF1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6. Conclusion"
      ]
    },
    {
      "metadata": {
        "id": "FDi5TR_WYV0Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Their LSTM model successfully learned the blues form and was able to improvise over it. The LSTM could learn the form without melody, and could also learn to compose new melodies along with the chords. \"Much more work is warranted.\" They demonstrated that an RNN can capture local melody structures and long-term structures, which represents an advance for neural network music composition. The site for the paper's results is [here.](http://people.idsia.ch/~juergen/blues/)"
      ]
    },
    {
      "metadata": {
        "id": "6j0ZWxAyYy3F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-Fin 1/10/2019-"
      ]
    },
    {
      "metadata": {
        "id": "nzezk5DAY5tk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Source 4: [Learning to Create Jazz Melodies Using Deep Belief Nets](https://www.cs.hmc.edu/~keller/jazz/improvisor/ICCCX-Bickerman-Bosley-Swire-Keller.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "-fQSSrmlaWBe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Abstract** - This paper describes an unsupervised learning technique which automatically creates jazz improvisation over chord sequences. They trained deep belief nets, which are based on restricted Boltzmann machines. They present their encoding scheme, the specifics of learning, and the creation process for their resulting music. Their model created novel jazz licks and should be regarded as a feasibility study for whether such networks could be used at all. Clearly, maybe."
      ]
    },
    {
      "metadata": {
        "id": "vXl_d0gNaTaG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "metadata": {
        "id": "YejqRMfpbCBc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Due to the structural nature of chord progressions, it's feasible that a machine could be taught to emulate human jazz improv. This could be done by stating rules (jazz grammars), but these risk losing the flexibility or fluidity of jazz. Instead of giving an algorithm rules, what if we give it stylistic examples of what we'd like to hear more of? It can determine the features present on its own."
      ]
    },
    {
      "metadata": {
        "id": "3P1UXVIibjWd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Their approach was **deep belief networks**, or DBNs. These are multi-layer restricted Boltzmann machines (RBMs), which are a type of stochastic (a given input gives a somewhat random output) neural network. They merely focus on the creation of melodies, not at all in a real-time collaborative setting. This is application-based, nothing more."
      ]
    },
    {
      "metadata": {
        "id": "yW_-jonAcQNO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "They believed DBNs might work due to recent work of Hinton, as of 2010. DBNs learn to recgonize by creating examples (as bit vectors) and comparing those to training examples. The model then adjusts its parameters to produce more similar examples and is thus unsupervised. This is similar to the way humans emulate behavior. The stochastic part of DBNs helps them to achieve novelty, and thus almost creativity. Their goal is create interesting melodies as opposed to Hinton's of recognizing them."
      ]
    },
    {
      "metadata": {
        "id": "rzFiwA6Jd7i4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Restricted Boltzmann Machines"
      ]
    },
    {
      "metadata": {
        "id": "CoKlCps-eAkI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The **restricted Boltzmann machine** (RBM) was introduced by Smolensky in 1986 and developed by Hinton in 2002-2007. It has two layers of neurons: one visible and one hidden layer. The two layers are fully connected with symmetric, bi-directional weights."
      ]
    },
    {
      "metadata": {
        "id": "5jpj1xW3a0w5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One training cycle for the algorithm takes a binary data vector as input , activating the visible neurons to match the input data. It then alternates activating the hidden nodes based on the visible nodes and vice versa. Each node is activated probabilistically based on a weighted sum of the nodes connected to it. Because nodes within a layer are not connected to other nodes in the layer, the activation of each layer only depends on the other layer. Once the network stabilizes, the new configuration of visible nodes can be read as the output."
      ]
    },
    {
      "metadata": {
        "id": "0Y4jGdoZbgcL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The goal of an RBM is to learn the features of sets of data sequences. This paper implemented the *contrastive divergence* (CD) algorithm based on [Hinton's work](http://www.cs.toronto.edu/~fritz/absps/tr00-004.pdf). Their implementation was modeled based on a [tutorial by Radev](http://imonad.com/rbm/restricted-boltzmann-machine/). The CD enabled inexpensive training given the large number of nodes and weight in their network. Once trained, an RBM can take random data sequences and generate new sequences which emulate the features of the training data."
      ]
    },
    {
      "metadata": {
        "id": "hWtHHnbtdKgU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A single RBM can learn some patterns in training data, but multiple RBMs layered together are even more powerful. Such a machine is called a **deep belief network**. The RBMs are combined by identifying each one's hidden layer as the visible layer of the next.  \n",
        "![DBN Graphic](https://i.imgur.com/TiMcbrv.png)"
      ]
    },
    {
      "metadata": {
        "id": "NTcsCNpkeRqD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The second RBM learns features about the features learned by the first, and so on. Thus the DBN can learn far more intricate patterns than a single RBM could alone."
      ]
    },
    {
      "metadata": {
        "id": "AA3NVAoBcZky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Data Representation"
      ]
    },
    {
      "metadata": {
        "id": "uGNibVuNfELw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To train RBNs on musical data, the authors had to encode the music as *bit vectors*, with each beat divided into *slots*. They chose 12 slots per beat to allow both 16th notes and triplets. Each slot ended up as a block of 30 bits, with 12 chord bits and 18 melody bits."
      ]
    },
    {
      "metadata": {
        "id": "6TvTqKokgn3N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the melody, they one-hot encoded the twelve note options as twelve slots and then encoded four more slots to specify one of four octaves. One bit indicated a sustained note and one represented rests. When a note is attacked, only its pitch and octave bits were on. If it's being sustained, only the sustain bit is on. Doing their octaves like this saved training time by reducing the number of bits quite a lot.  \n",
        "![Pitch Encoding](https://i.imgur.com/m7CE4xG.png)"
      ]
    },
    {
      "metadata": {
        "id": "JHt6DSKXhbJc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each chord was one-hot encoded as the 12 possible pitches in the chord. These were simply on-off bits. The melody and chord vectors were concatenated to form part of the input corresponding to one slot. Hopefully, the machine learns to associate specific chords with melodic features."
      ]
    },
    {
      "metadata": {
        "id": "n__qnlEYivki",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Training Data"
      ]
    },
    {
      "metadata": {
        "id": "oVI5CHl8ixvD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The network was initially trained on a small set of children's melodies. These were all in the same key and consisted of simple rhythms and notes all in their respective chords. Once they had trained a model to create these melodies, they moved on to larger networks and jazz."
      ]
    },
    {
      "metadata": {
        "id": "3h07WIbAjKg0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Their main dataset was a large corpus of 4-bar jazz licks (short coherent melodies) cycling over the common $\\text{ii}-\\text{V}-\\text{I}-\\text{VI}^7$ turnaround progression in a single key. The $\\text{ii}-\\text{V}-\\text{I}$ cadence is common to jazz, and the $\\text{VI}^7$ is the dominant to the next $\\text{ii}$ chord. The licks were either transcribed from notable jazz solos or hand constructed, some with help from a \"lick generator\" from the [Impro-Visor software](https://www.cs.hmc.edu/~keller/jazz/improvisor/). "
      ]
    },
    {
      "metadata": {
        "id": "vUzZHqdAkm2H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. Learning Method"
      ]
    },
    {
      "metadata": {
        "id": "WbJlUahekrvc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Their goal was to create melodies which transitioned between chords in the progression. To aid this effort, they trained the model on windows of one measure each which cycled forward one beat at a time. These measure-long snapshots continued until the end of the 4-bar lick. In this way, a single training example was broken into 13 overlapping 1-bar windows."
      ]
    },
    {
      "metadata": {
        "id": "v5J6-TGBoyfH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For creating melodies, they started the model with a \"seed\" of specified chord bits of the desired progression and random melody bits. The chord bits are *clamped* so that they cannot be modified during the creation cycle.In creating the melody, they use a process similar to the windowing used for training. They generate the first few beats of the melody, clamp their bits, and then shift down the melody and chords to make room for the next beat. Thus the machine generates one beat at a time but uses clamped chords and clamped beats of preceding melody.  \n",
        "![Clamping Process](https://i.imgur.com/k6aZaIG.png)"
      ]
    },
    {
      "metadata": {
        "id": "L_XfY7ATp4ZZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "During the final activation of the network's visible layer (which will be the newly generated melody), they constrain the activation of bits in a specific way. Each slot is looked at individually and only the highest probability of pitch and octave is used. Thus the machine chooses one of sustaining, resting, or starting a new pitch. This approach was found to create a variety of melodies while resonating well with the chords."
      ]
    },
    {
      "metadata": {
        "id": "8BT7F5mBu4zy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "They also tested if the machine could learn the progression in arbitrary keys and thus included the option to transpose each input into different keys. All functionality mentioned was implemented in a stand-alone tool they call \"[RBM-provisor](https://sourceforge.net/projects/rbm-provisor/).\" The tool was written in Java and supports input/output in the leadsheet format from Impro-Visor."
      ]
    },
    {
      "metadata": {
        "id": "bZBdiEiOpeXm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6. Results"
      ]
    },
    {
      "metadata": {
        "id": "1i15roSdv9Y3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Their first experiments used children's melodies with 2-layer networks trained for 100 epochs. The results were successful, fitting chords well and flowing melodically. After this success, they moved on to jazz."
      ]
    },
    {
      "metadata": {
        "id": "3OkYKNxlkgpt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the jazz creation networks, they experimented with the model, varying the number of layers, number of neurons per layer, number of training epochs, and other factors. They settled on a 3-layer network with 1441 input nodes (4 beats with 12 slots each  and 30 bits per slot plus one bias slot) and 750, 375, and 200 hidden nodes respectively. Typical training lasted 250 epochs on about 100 4-bar licks. This took about nine hours on an inexpensive desktop computer."
      ]
    },
    {
      "metadata": {
        "id": "SORT0U35lXo-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When analyzing their generated music, they found that most notes were in the chord, with some color tones. Foreign tones were quite rare. Created melodies tended to avoid large jumps and rarely jumped octaves."
      ]
    },
    {
      "metadata": {
        "id": "BYSnk-SblzrM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Output Example](https://i.imgur.com/P0ZYODm.png)"
      ]
    },
    {
      "metadata": {
        "id": "1o96aa0zmVSi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The training method was tested with transpositions by training on four copies of the inputs, each transposed up 0, 1, 2, or 3 semitones. The machine was still able to create chord-compatible music regardless of the seed chords. They didn't test with more than 4 transpositions due to training time."
      ]
    },
    {
      "metadata": {
        "id": "7e2V0aJSm1c9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To justify the importance of being able to transpose, jazz chord progressions often have abrupt implied key changes. Thus relative transpositions quickly become important. As an example, it's more economical to train on all transpositions of a $\\text{ii}-\\text{V}-\\text{I}$ than all possible contexts of any one version of the progression."
      ]
    },
    {
      "metadata": {
        "id": "26gZTAMInc37",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "They noticed some differences between the training data and their generated music:\n",
        "1. Generated licks tended to avoid half-step intervals. This avoidance of approach notes tended towards chord tones.\n",
        "2. Rhythmically, the outputs almost entirely consisted of duplet rhythms."
      ]
    },
    {
      "metadata": {
        "id": "NdPBVH1voBFC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An alternative approach was attempted to select bits using the neuron probability distribution, as opposed to the maximum probability. This produced the disjointed melodies seen above. They also tried encoding strong/weak beat information, but these results were not any better than the chosen encoding."
      ]
    },
    {
      "metadata": {
        "id": "uqjQAM6_ojv0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To be clear, DBNs are not the author's first choice for lick generation. The approach used by Impro-Visor is superior to this one, by far. DBNs also take a while to train. It is possible, though, that DBNs avoid as much algorithmic bias as unsupervised approaches using clustering and Markov chains."
      ]
    },
    {
      "metadata": {
        "id": "nGyrc4bdpFLj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7. Future Work"
      ]
    },
    {
      "metadata": {
        "id": "bSmilRC3pGmq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These results are promising, certainly, but further improvements could be made. Their model mainly output duplet rhythms, despite triplets in the training data. This is likely due to an overshadowing of the triplets, but no alternative note generation rule yet found yields as coherent results."
      ]
    },
    {
      "metadata": {
        "id": "gP2gH32apj7Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The generated music also included a disproportionate number of repeated notes, which sound a bit static and immobile. They did try post-processing generated music to merge repeated notes, but better solutions would prevent the machine from producing the notes in the first place. Perhaps a different encoding?"
      ]
    },
    {
      "metadata": {
        "id": "8rG-QteiqDS3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "They also believe that a similar method could be used to infer chords. Theirs took chords as an input and produced suitable melodies. Another DBN work might be able to determine possible chord progressions for a given melody."
      ]
    },
    {
      "metadata": {
        "id": "5MuH4OSzqZJ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 8. Related Work"
      ]
    },
    {
      "metadata": {
        "id": "VY-xZqG9qodZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Rossen Radev's tutorial on RBM implementation is again cited. Previous work into music generation has largely already been mentioned in this notebook above."
      ]
    },
    {
      "metadata": {
        "id": "SQ4aTHCLrkkX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 9. Summary"
      ]
    },
    {
      "metadata": {
        "id": "uzC0cY1krmuN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These results show that DBNs can learn to create jazz licks. The approach can work in multiple keys and suggests that, given a significant dataset, similar models might be able to solo over full 12-bar progressions. Their results show novelty but do have minor limitations in rhythm and pitch repetition. The possibility of using DBNs in this way was proven, and further work is likely to be done."
      ]
    },
    {
      "metadata": {
        "id": "Ch1vrR68sK2X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-Fin 1/12/19-"
      ]
    },
    {
      "metadata": {
        "id": "nkcjmRaltf-C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Source 5: [Composing a melody with LSTM RNNs](https://pdfs.semanticscholar.org/f707/ff253dc44ffa1e15f7ad19d75473a3ddecac.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "dQFfmCWMts-5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This source, unlike the others, is 56 pages in full. As such, I'm going to read through it a bit differently by reading full sections and then looping back to take notes."
      ]
    },
    {
      "metadata": {
        "id": "I_QBEYVouCiz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "metadata": {
        "id": "ElRL29zZuLFx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This paper's primary goal will be to see to what extent LSTM networks can be used to generate melodies over chord progressions. It will also explain similar prior attempts, go over the algorithms used, explain the implementations for this particular result, and evaluate the generated music by comparing it to music composed by humans using judging by human subjects."
      ]
    },
    {
      "metadata": {
        "id": "ThGV_a0EwCyO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. State of the Art in Algorithmic Composition"
      ]
    },
    {
      "metadata": {
        "id": "us_in8NewVRk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are approaches dating back to 1024AD for algorithmic composition, but the advent of computers enabled many more possibilities. Starting in 1955 with the Illiac Suite and Metastasis, algorithmic composition continued with the likes of Cope's Experiments in Musical Intelligence (1981), Mozer's CONCERT (1994), and the LSTM work of Eck and Schmidhuber in 2002. This thesis was motivated to pick up where Eck left off."
      ]
    },
    {
      "metadata": {
        "id": "IJWg_omOY3Yi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Neural Networks"
      ]
    },
    {
      "metadata": {
        "id": "TsRGt7SL2yqI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Feedforward Networks"
      ]
    },
    {
      "metadata": {
        "id": "HtY1yfsXY5ZE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Feedforward Neural Networks Basics**  \n",
        "These networks are comprised of neurons, which sum numerous inputs and squash the result using some activation function. For any one neuron $k$, each input $x_i, i=1,2,...,m$ is multiplied by its own weight $w_{ki}$. The net input $v_k$ is the weighted sum over all inputs."
      ]
    },
    {
      "metadata": {
        "id": "jN3f0EqJaLG_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There's also a single bias $b_k$ feeding into the neuron. This bias can be set to weight $w_{k0}$, where $x_0=1$. We can then set $v_k=\\sum_{i=0}^mw_{ki}x_i$."
      ]
    },
    {
      "metadata": {
        "id": "R8g2eFtrao0B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The final output, or activation $y_k$, of the neuron applies some activation function $\\varphi(\\cdot)$ to net input $v_k$:  \n",
        "$y_k=\\varphi(v_k)$."
      ]
    },
    {
      "metadata": {
        "id": "SRPUkXc7bw45",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Of the many activation functions used, the $\\text{sigmoid}$ function is quite common. It squashes outputs of the neuron between 0 and 1:  \n",
        "$\\sigma(v_k)=\\frac{1}{1+e^{-a*v_k}}$, where slope parameter $a$ can vary as needed."
      ]
    },
    {
      "metadata": {
        "id": "eyw11GF2ckGN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Network Architecture**  \n",
        "A given number of these neurons are in each row of a neural network. Each row is fully connected to the next by *synapses*, which each have the weights we're familiar with. The first layer in a network is called the *input layer* and the last is the *output layer*. In networks with more than two layers, middle layers are called *hidden layers*.  \n",
        "![Multi-layer Network](https://i.imgur.com/PgIZUrL.png)"
      ]
    },
    {
      "metadata": {
        "id": "mtTwPHvweEL3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Learning through Backpropagation**  \n",
        "These networks need some way to learn, to change their synaptic weights to output closer to the desired results. They'll learn from training data ${(x^{(1)},t^{(1)}),(x^{(2)},t^{(2)}),...,(x^{(n)},t^{(n)})}$, with inputs $x^{(n)}$ and expected targets $t^{(n)}$, where $n$ is the number of examples."
      ]
    },
    {
      "metadata": {
        "id": "RF1MZtvvgBia",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We need some *loss function* to evaluate the difference between the target values $t_j$ and the actual network outputs $y_j$. The squared error function is often used for this, where $j$ refers to the $j$-th neuron of the $J$ in the output layer.  \n",
        "$E(w_{ki})=\\frac{1}{2}\\sum_{j\\in{J}}(y_j-t_j)^2$."
      ]
    },
    {
      "metadata": {
        "id": "KdYWphVehDcQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once we have this metric to see how far off we are, we still need to update our weights to improve the network's output. This goal is synonymous with minimizing the loss, and thus we can use the *gradient descent* method introduced [in 1986](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf). The general update rule of this method follows:  \n",
        "$w_{ki}:=w_{ki}-\\alpha\\frac{\\delta}{\\delta(w_{ki})}E(w_{ki})$, with learning rate $\\alpha$."
      ]
    },
    {
      "metadata": {
        "id": "Roqr1Z9oi6pM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Computing Partial Derivatives**  \n",
        "This formula calls for the calculation of the partial of $E(w_{ki})$ w.r.t. weights $w_{ki}$. This is done in two steps: one for the output layer and one for the hidden layers."
      ]
    },
    {
      "metadata": {
        "id": "U8LAN51ckc5o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Weights in Output Layer**  \n",
        "By the definition of $E$, we have $\\frac{\\delta{E}}{\\delta{w_{ji}}}=\\frac{\\delta}{\\delta{w_{ji}}}\\frac{1}{2}\\sum_{j\\in{J}}(y_j-t_j)^2$.  \n",
        "$=> \\frac{\\delta{E}}{\\delta{w_{ji}}}=(y_j-t_j)\\frac{\\delta}{\\delta{w_{ji}}}y_j$. This is because the partial of $E$ w.r.t. one $w_{ji}$ will only involve that specific $j$; all other terms of the sum will be zero. The chain rule explains the rest, as $t_j$ is constant."
      ]
    },
    {
      "metadata": {
        "id": "ycYBnuatlSr0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Recall that we had $y_k=\\varphi(v_k)$.  \n",
        "Then $\\frac{\\delta{E}}{\\delta{w_{ji}}}=(y_j-t_j)*\\frac{\\delta}{\\delta{w_{ji}}}\\varphi(v_j)$. Chain rule again to get:  \n",
        "$\\frac{\\delta{E}}{\\delta{w_{ji}}}=(y_j-t_j)*\\varphi'(v_j)*\\frac{\\delta}{\\delta{w_{ji}}}v_j$."
      ]
    },
    {
      "metadata": {
        "id": "r3LhmtCLu2_G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Because $v_k=\\sum_{i=0}^mw_{ki}x_i$, we have $\\frac{\\delta}{\\delta{w_{ji}}}v_j=x_i$ by all other terms equalling zero.  \n",
        "$=> \\frac{\\delta{E}}{\\delta{w_{ji}}}=(y_j-t_j)*\\varphi'(v_j)*x_i$."
      ]
    },
    {
      "metadata": {
        "id": "XhBqhpe2veT1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the sake of simplicity we also define $\\delta_j:=(y_j-t_j)*\\varphi'(v_j)$. Finally:  \n",
        "$\\frac{\\delta{E}}{\\delta{w_{ji}}}=\\delta_jx_i$."
      ]
    },
    {
      "metadata": {
        "id": "Rd0zprGOwDgx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Weights between Hidden Layers**  \n",
        "We now consider weights $w_{ki}^{(l)}$, where such a weight connects the $i$-th neuron in layer $l-1$ to the $k$-th neuron in hidden layer $l$. We unfortunately cannot omit the sum this time, as each $y_j$ depends on all previous weights in the network."
      ]
    },
    {
      "metadata": {
        "id": "dwZQ5wC1xDP4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We again start with $\\frac{\\delta{E}}{\\delta{w_{ki}}}=\\frac{\\delta}{\\delta{w_{ki}}}\\frac{1}{2}\\sum_{j\\in{J}}(y_j-t_j)^2$.  \n",
        "$=> \\frac{\\delta{E}}{\\delta{w_{ki}}}=\\sum_{j\\in{J}}(y_j-t_j)\\frac{\\delta}{\\delta{w_{ki}}}y_j$"
      ]
    },
    {
      "metadata": {
        "id": "pr7WKwDfxxjs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again, with $y_j=\\varphi(v_j)$ we have $\\frac{\\delta{E}}{\\delta{w_{ki}}}=\\sum_{j\\in{J}}(y_j-t_j)\\frac{\\delta}{\\delta{w_{ki}}}\\varphi(v_j)$.  \n",
        "$=> \\frac{\\delta{E}}{\\delta{w_{ki}}}=\\sum_{j\\in{J}}(y_j-t_j)\\varphi'(v_j)\\frac{\\delta}{\\delta{w_{ki}}}v_j$."
      ]
    },
    {
      "metadata": {
        "id": "8L9tJBSOypJp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I'm going to skip over this section for now. It's intense calculus and this is clearly worth fully studying on my own at some point. For now, I'd like to cover LSTMs to stay on-topic."
      ]
    },
    {
      "metadata": {
        "id": "nM0-WciY2vx7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RNNs"
      ]
    },
    {
      "metadata": {
        "id": "ISULxbQc2xOb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "RNNs can capture patterns over time, which is crucial to music production. Each neuron's output is fed back into it in each consecutive time step."
      ]
    },
    {
      "metadata": {
        "id": "hLVRjQxi3deN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Backpropagation Through Time**  \n",
        "Unfolding an RNN through time will result in a feedforward network. One can then apply a modified version of backpropagation to update the weights of the RNN."
      ]
    },
    {
      "metadata": {
        "id": "Fw3S3wsy4yTB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unfortunately, music composed with RNNs was found to be \"not musically coherent\" by Moser in 1994. This is due to the limited memory duration of the model, which is caused by a tendency towards exploding or vanishing gradients. LSTMs were introduced to solve this problem."
      ]
    },
    {
      "metadata": {
        "id": "ktJDp-gW487X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**LSTM RNNs**  \n",
        "LSTMs were designed to avoid the \"rapid decay of backpropagated error\" found in typical RNNs. This model uses a \"memory block\" (cell state) to store and recall memory. It has three gates which act as read, write, and reset functions. These gates are better discussed in my other notes specifically on the topic."
      ]
    },
    {
      "metadata": {
        "id": "IasPoNeJ5yGY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LSTMs can be trained with BPTT as well, although they need numeric data. Thus we'll need to represent music in this way, somehow."
      ]
    },
    {
      "metadata": {
        "id": "fua6n1U4583E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Data Representation: MIDI"
      ]
    },
    {
      "metadata": {
        "id": "RiO2Eazn6Buo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In choosing a representation for music to feed into the LSTM, we have the options of raw audio or MIDI. What are the benefits to each, you may ask? Let's check:"
      ]
    },
    {
      "metadata": {
        "id": "Ln41c-aN6bE-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our goal is to compose melodies for given chords, only caring about which notes ae held for what duration of time. We thus disregard velocity, articulation, or dynamics."
      ]
    },
    {
      "metadata": {
        "id": "SgoA5Tty6pUv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Audio** - It's a rather complex undertaking to reduce audio signals to their mere pitches. We'd need to Fourier Transform to get base frequencies, map frequencies, line up transients, and map timesteps."
      ]
    },
    {
      "metadata": {
        "id": "TPfrxxCA7CJt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**MIDI** - MIDI is the standardized data format for musical control data in digital instruments. MIDI only includes the start, duration, and end of pitches. It thus provides all we need for LSTMs and only needs to be converted into an appropriate numeric representation. Thus the author chose to use MIDI."
      ]
    },
    {
      "metadata": {
        "id": "Y5cWaBqdtP2_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Piano Roll Representation"
      ]
    },
    {
      "metadata": {
        "id": "o86lyl_ntR6W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This thesis used a representation similar to a piano roll, with a 2D matrix of 0s and 1s. MIDI ticks are 96 per beat and thus the data was upscaled to 4 ticks per beat. The vertical axis of the matrix is time and the horizontal the pitches. The range of the pitches included was only the notes in the training set. Overall the matrix was size $(\\text{num of }\\frac{1}{16} \\text{ steps},\\text{note range})$."
      ]
    },
    {
      "metadata": {
        "id": "s8CBfhrsuhdT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notes were indicated as \"on\" by a value of 1 and \"off\" by 0 for any one time step. This representation has the issue that multiple of the same note consecutively is identical to one longer sustained note."
      ]
    },
    {
      "metadata": {
        "id": "FbDnHl54vZfA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The author then adjusts their representation so that the second half of each note is denoted by 0. This leads to a need for two time steps for each note. For 4 ticks in a beat, this would only allow a resolution of eighth notes. Thus the final resolution is raised to 8 ticks per beat, which still allows that initial resolution of 16th notes."
      ]
    },
    {
      "metadata": {
        "id": "zuLBYfkOwPfv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. Implementation"
      ]
    },
    {
      "metadata": {
        "id": "b7Rd2AKjwRwX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The author chose to use Keras in Python. Mido was used to deal with the MIDI data. Overall, the implementation had two programs:\n",
        "1. Training the LSTM.\n",
        "2. Composing a melody for a new chord sequence.  \n",
        "![The Two Programs](https://i.imgur.com/RKzTqaW.png)"
      ]
    },
    {
      "metadata": {
        "id": "Ku700svMArj-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Training Program"
      ]
    },
    {
      "metadata": {
        "id": "oFjjf5hC6si-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Network Inputs**  \n",
        "The training dataset consists of chord sequence inputs and the belonging melodies as targets. The MIDI has 96 ticks per beat, and all times in the MIDI encoding are relative to each other; there is no absolute time. In this way the melody an chords are turned into the matrices."
      ]
    },
    {
      "metadata": {
        "id": "oDtUsAjU8Ibo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For Keras, the inputs need further transformation. During training, several timesteps of chords will form a 2D matrix. The network will then output a vector predicting which notes to play next."
      ]
    },
    {
      "metadata": {
        "id": "kH2hwm2P9kyB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The number of timesteps in the chord sequence is denoted by $n$. The target vector is then the $n+1$ timestep from the melody piano roll."
      ]
    },
    {
      "metadata": {
        "id": "N2-e2QoS-AKj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Keras framework LSTM requires 3D input matrices of size $(\\text{num samples}, \\text{timesteps}, \\text{input dimension})$ and 2D target matrices of size $(\\text{num samples}, \\text{output dimension})$. Here, $\\text{timesteps}$ refers to the sequence length $n$. The $\\text{input dimension}$ is the number of pitches possible for the chords. The $\\text{output dimension}$ is similar for the melody's pitch range. The number of samples is the difference between the number of total timesteps in the song and the sequence length."
      ]
    },
    {
      "metadata": {
        "id": "zIJUmR5s_HSa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Network Input Matrices are created by taking a sample of size $(n, \\text{chord pitch range})$ from the start of the chord piano roll. The following samples are then created by scrolling the window of length $n$ along the matrix timestep by timestep. This goes until the window includes the penultimate timestep of the matrix. The Prediction Target Matrices take the melody piano roll at each timestep right after the Network Input Matrices."
      ]
    },
    {
      "metadata": {
        "id": "GrtMdBUiAICk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Personal notes: this encoding used 8 timesteps per beat, and mine needs 24 with the current time scale I'm allowing if I use this way to end notes. I'd need $n=24$ to replicate his scale of LSTM inputs, so maybe I should consider other encoding methods."
      ]
    },
    {
      "metadata": {
        "id": "oJ7HLoWbAyeR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Network Properties**  \n",
        "An LSTM RNN consists of an input layer, output layer, and optional hidden layers. The input layer includes input nodes fully connected to the next layer. This architecture used 12 input nodes and 24 output nodes. This gives an octave for chords and two octaves for melodies. The program leaves it up to the user to vary up the number of hidden layers and their contents.  \n",
        "![Possible LSTM Structure](https://i.imgur.com/z0W6Smb.png)"
      ]
    },
    {
      "metadata": {
        "id": "81EfAI2NCRRQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Composition Program"
      ]
    },
    {
      "metadata": {
        "id": "qBLBlcmZCXAc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After the LSTM is trained, it can be used to compose new melodies for given chord sequences. This process starts with a given MIDI chord progression compressed into one octave. We can then piano roll this MIDI, turn it into a Network Input Matrix, and feed this into the LSTM."
      ]
    },
    {
      "metadata": {
        "id": "1_dd4vR9C03j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model will predict output values for each timestep example of the input. The resulting Prediction Matrix will consist of values between 0 and 1. At each timestep, the highest value is found. If this value is higher than a given value, it's set to 1 and otherwise all values in the timestep are set to 0. We thus get a monophonic melody piano roll."
      ]
    },
    {
      "metadata": {
        "id": "XSNT2r3FDpEW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We then need to create MIDI from the piano roll. This is done in a reverse of the process used to create the piano rolls initially. This concludes the composition program."
      ]
    },
    {
      "metadata": {
        "id": "Urld3i3SD-QD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6. Experiments"
      ]
    },
    {
      "metadata": {
        "id": "nAr5vbYQD_u-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section we'll describe the training and testing data and then explain the experimental training process."
      ]
    },
    {
      "metadata": {
        "id": "shkHh3bAEkN1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training/Test Datasets"
      ]
    },
    {
      "metadata": {
        "id": "ydHaHQRLEn_i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training Data**  \n",
        "The decision for what to use as training data is a crucial one, for this in itself defines the compositional goal of the network. This thesis had two goals for LSTMs:  \n",
        "1. Compose music indiscernible from human-composed music.\n",
        "2. The melodies should simply sound pleasant."
      ]
    },
    {
      "metadata": {
        "id": "o0iujzmOnExj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This second metric is difficult to measure, so the author chose to use music appealing to a broad demographic: The Beatles. The author's justification was pretty obvious: their success and popularity. The results also needed to be short in duration, as test subjects would be listening to multiple examples. This was set to 8 bars."
      ]
    },
    {
      "metadata": {
        "id": "uOZ9A4q-E6iv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The training data was taken from 16 different Beatles songs from the book \"Pop Classics For Piano: The Very Best Of The Beatles - Easy Arrangements for Piano.\" 68 8-bar samples were taken from these songs. The chords' notes ranged from C2 to B2, and the melodies from C3 and B4. Quantization was to the 16th note."
      ]
    },
    {
      "metadata": {
        "id": "hG0UMSwqo76c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Test Data**  \n",
        "This was a set of previously unseen chord sequences which the LSTM would compose melodies over. 4 of the 8 test progressions included chords which weren't in the training dataset, while the other 4 were comprised of the most common chords.  \n",
        "![Test Chord Progressions](https://i.imgur.com/5UmiNnZ.png)"
      ]
    },
    {
      "metadata": {
        "id": "GOF6022BpuHJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training Different LSTM Topologies"
      ]
    },
    {
      "metadata": {
        "id": "KEXTNjVCp3xi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There no existing heuristics to decide the optimal topology for LSTMs and music composition. The author therefore chose the best topology out of eleven based on its compositions on training data."
      ]
    },
    {
      "metadata": {
        "id": "zFNWE3aLqNmi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The networks tested included zero to four hidden layers. The number of LSTM blocks per layer was chosen as multiples or fractions of the number of input nodes (12). The experiment started with 0 hidden layers, and added hidden layers as it went. As hidden layers were added, the melodies were more and more cohesive up until networks with 4 layers were more monotonous. Thus training stopped after 4 hidden layers."
      ]
    },
    {
      "metadata": {
        "id": "C5J7Pvjpq9aM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Topologies Tested](https://i.imgur.com/QbSSpf8.png)"
      ]
    },
    {
      "metadata": {
        "id": "v7cD-bdmrtR-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Network Compositions"
      ]
    },
    {
      "metadata": {
        "id": "7ZXpcCNZrvM6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each network composed melodies to the eight test chord sequences. The LSTMs with more hidden layers played fewer notes, in general, until the 4-layer LSTM was often stuck on a single note or not playing at all. The LSTMs in general learned to play diatonically to the key."
      ]
    },
    {
      "metadata": {
        "id": "jNx8vESYsXsO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One issue was that the networks often played notes off the beat, either an 8th or 16th after what might be expected. Thus the beginning and ending of all notes were adjusted to the nearest 8th note."
      ]
    },
    {
      "metadata": {
        "id": "Mjy0zbBassh4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Of the 11 topologies, 3 were pre-selected and tested in subjective listening tests with 3 participants. The seventh network topology's melodies were selected as the most pleasant and human-sounding. These melodies were used in a listening test."
      ]
    },
    {
      "metadata": {
        "id": "373jlgxPtAqK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "lg2_IcE5tCW9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The melodies which the seventh LSTM produced were tested in an evaluation similar to the framework presented by [Pearce](http://webprojects.eecs.qmul.ac.uk/marcusp/papers/PearceEtAl2001.pdf). The two compositional goals were tested with human listeners. The test was developed to be taken online and 109 people participated. The test was broken into two parts:"
      ]
    },
    {
      "metadata": {
        "id": "eHK0nzS0tqL4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Part 1**  \n",
        "The author wanted to determine if humans prefered LSTM compositions or human ones. Four \"musically skilled persons\" were each asked to compose melodies over two of the eight test chord progressions. Each pair of LSTM-human melodies over the same chord progression was presented as a pair. Participants were asked which melody they liked more.\n"
      ]
    },
    {
      "metadata": {
        "id": "yqHD65_BuHPD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Part 2**  \n",
        "This section aimed to determine if the LSTM compositions are recognizably not by humans. At this point, participants were told some melodies were written by a computer. Given one of the sixteen melodies, they were asked if each was written by a human or computer. At the end they were also asked for reasons for their decisions."
      ]
    },
    {
      "metadata": {
        "id": "ynfxBE4ZwQC_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Participants were also asked about themselves to get general imformation about their musical background. The factors asked about were based on [this paper from 2014](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0089642)."
      ]
    },
    {
      "metadata": {
        "id": "RUZtIeCIwjG7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Results**  \n",
        "The participants prefered human compositions to computer-generated ones, although the computer melodies could not be identified as such on their own. The demographic was largely quite musical (64.22% played instrument for >3 years). The LSTM melodies were prefered about 26% of the time on chord sequences 1-4, and about 29% on sequences 5-8. Maybe the slight gap is due to forced creativity?"
      ]
    },
    {
      "metadata": {
        "id": "OIt_9Fc6xqTp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Overall, the LSTM managed to create prefered melodies in 27% of cases. That's not bad. The computer melodies were correctly identified about 60 of the time, which is again not bad."
      ]
    },
    {
      "metadata": {
        "id": "T96bQ7JhyVP7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 8. Conclusion"
      ]
    },
    {
      "metadata": {
        "id": "VmXYHDu5yWwq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The LSTM managed to somewhat meet the goals for the thesis, although improvements could be made. Possible future improvements are suggested:\n",
        "* More training data\n",
        "* More acute compositional goals and tailored data for these\n",
        "* Another representation style"
      ]
    },
    {
      "metadata": {
        "id": "tK3PcNrJzTYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-Fin 1/15/19-"
      ]
    },
    {
      "metadata": {
        "id": "yRZiKSwghESY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Source 6: [Music Generation from MIDI datasets](https://neuro.cs.ut.ee/wp-content/uploads/2018/02/MIDI_music.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "wdLvp-7lhTls",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Abstract** - This work employed a character-based RNN to generate music. In previous works, RNNs have succeeded in modeling sequential data."
      ]
    },
    {
      "metadata": {
        "id": "yAOOOBZKhiB9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "metadata": {
        "id": "Nezkc_x3hkZ_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This project's aim was to replicate some previous approaches in using RNNs. Prior to this paper, RNNs as well as CBOW, sequence-to-sequence, or GANs have been tried for music generation. This paper used MIDI data quantized to specific note lengths, and then specified one character for each note when it's played."
      ]
    },
    {
      "metadata": {
        "id": "zQjRBQQKiS5D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2. Background"
      ]
    },
    {
      "metadata": {
        "id": "NDAor3pciUM-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Machine improvisation is a relatively new concept which uses AI and ML to capture the characteristics of music. These models use available examples and glean insight from them. Especially of note are neural models, which are layered computational graphs. In each layer of these graphs, more sophisticated higher level features are represented. For music generation, LSTM cells are even more powerful."
      ]
    },
    {
      "metadata": {
        "id": "7JmDz8fsi7Aq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The paper by N. Agarwala generated monophonic and polyphonic music using multiple methods. Their RNN reached 60% accuracy and sequence-to-sequence reached 65% accuracy. The latter fooled 75% of humans if the compositions were by human or machine. They also experimented with GANs, but no useful results were found."
      ]
    },
    {
      "metadata": {
        "id": "57qY2uHljZVT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agarwala found that char-RNNs benefitted from increased hidden layer size and shorter embedding size, while seq-to-seq benefitted from increasing hidden layer and embedding size. They used ABC notation, as MIDI requires less of a learning system. Thus ABC creates models able to learn the grammar of music."
      ]
    },
    {
      "metadata": {
        "id": "jRcbIbKzj64s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bob L. Sturm used character- and token-based approaches with LSTMs. Sturm's work also created a statistical analysis technique which helps relate LSTM compositions to originals. This gives a recipe for regularizing the RNN models to generate grammatical music."
      ]
    },
    {
      "metadata": {
        "id": "vHy2Sn-UkT-h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "--to be continued--"
      ]
    }
  ]
}