{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summaries",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "qA1Ycmlt5jC5",
        "hbpRoMjp9yWc"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benendead/LSTMjazz/blob/master/Research/Summaries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "P0FU2Tx1-kZ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Author(s) | Year | Models Used | Task | Useful lessons | Future work\n",
        "--- | ---\n"
      ]
    },
    {
      "metadata": {
        "id": "HtIUKuFX-QLO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Topics Left To Study** (Paper Mentioned In)  \n",
        "* RTRL error (Eck 2002)\n",
        "* BPTT (Eck 2002)\n",
        "* Kalman filter to update weights? (Eck 2002)\n",
        "* [RBM](http://imonad.com/rbm/restricted-boltzmann-machine/) (Bickerman 2010)\n",
        "* [Contrastive Divergence](http://www.cs.toronto.edu/~fritz/absps/tr00-004.pdf) (Bickerman 2010)"
      ]
    },
    {
      "metadata": {
        "id": "5uGIymurBuLz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Summaries"
      ]
    },
    {
      "metadata": {
        "id": "qA1Ycmlt5jC5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Choi et al. (2016)"
      ]
    },
    {
      "metadata": {
        "id": "aS2l3cVl5lA4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Summary**  \n",
        "They used char-RNN and word-RNN to generate jazz chord progressions. Their data was 2,486 scores transposed to C from real books. The models were fed text data of chord progressions, with all quarter notes filled in, as in:  \n",
        "-START- F:9 F:9 F:9 F:9 D:min7 D:min7 G:9 G:9  \n",
        "C:maj C:maj F:9 F:9 C:maj C:maj C:maj C:maj -END-  \n",
        "There were 39 distinct characters, 1,259 distinct chords, and 539,609 chords overall."
      ]
    },
    {
      "metadata": {
        "id": "yowVS08H7rQc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Why char-RNN?**  \n",
        "Benefits of char-RNN:\n",
        "* Smaller vocabulary\n",
        "* Fewer assumptions about music made\n",
        "\n",
        "Downsides of char-RNN:\n",
        "* Shorter effective length of memory, as more time steps are needed to input the same information"
      ]
    },
    {
      "metadata": {
        "id": "fXX0MAOW64rf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Model Architecture**  \n",
        "Two LSTM layers with 512 hidden units (hidden state dimensionality) and then Dropout of 0.2 after each LSTM layer. Put together in Keras with categorical cross entropy as loss, Adam optimizer, and stochastic prediction based on a diversity parameter $\\alpha$. New probabilities are calculated:  \n",
        "$\\hat{p}_i=e^{log(p_i)/\\alpha}$, where $p_i$ is the probability for the $i$ states.  \n",
        "A state is then selected based on the probabilities."
      ]
    },
    {
      "metadata": {
        "id": "Bk1HWxDS8_1L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Successes**  \n",
        "Their char-RNN learned $\\text{ii}-\\text{V}-\\text{I}$, passing chords, modal interchange chords, and tritone substitutions. The word-RNN included modal interchange, circle of fifths progressions, and descending basslines. Their code is available [here](https://github.com/keunwoochoi/lstm_real_book).  \n",
        "*-Fin 1/24/2019-*"
      ]
    },
    {
      "metadata": {
        "id": "hbpRoMjp9yWc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Eck et al. (2002)"
      ]
    },
    {
      "metadata": {
        "id": "KVLPVy2b99J2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Summary**  \n",
        "This paper ran two experiments with LSTMs:\n",
        "1. Memorizing a chord progression.\n",
        "2. Also improvising pentatonic blues over a memorized progression.  \n",
        "\n",
        "Both experiments were successful using relatively simple LSTMs. They represented the data as a vector with 12 possible chord notes and 13 melody notes. Their quantization step was an eighth note and note representation was a simple 1/0 on/off input.  \n",
        "\n",
        "Examples can be found [here](https://web.archive.org/web/20190104192500/http://people.idsia.ch/~juergen/blues/)."
      ]
    },
    {
      "metadata": {
        "id": "F0ByvqcJBxjz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Experiment 1**  \n",
        "The data was just the same progression repeatedly. The network architecture used 4 LSTM cells each with 2 hidden cells all fully connected to each other and the input layer. Forget, input, and output gate biases were set to -0.5, -1.0, -1.5, and -2.0 for the four blocks, respectively. Output biases were 0.5, learning rate 0.00001, momentum rate 0.9. Loss was cross-entropy and output function was logistic sigmoid."
      ]
    },
    {
      "metadata": {
        "id": "_Vr96JioClsz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the output, chord notes were on if they had a probability above 0.5. Training ended once the chord sequence was completely memorized."
      ]
    },
    {
      "metadata": {
        "id": "rc8TWaKoDOCf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Experiment 2**  \n",
        "Some cell blocks learned melody and others learned chords. There were eight cell blocks total with two hidden cells each. Melody information never reached chord cell blocks, although everything else was identical to Experiment 1."
      ]
    },
    {
      "metadata": {
        "id": "Q34CIwpqD0Zy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again, chord outputs had a 0.5 threshold. The melody output needed to pick one note, so the probabilities were constrained to sum to 1 and then a random number in [0,1] was used. Training ended until objective error plateaued. Outputs were seeded and then allowed to compose progressions.  \n",
        "*-Fin 10/24/2019-*"
      ]
    },
    {
      "metadata": {
        "id": "6YXHNtMTE3KI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Bickerman et al. (2010)"
      ]
    },
    {
      "metadata": {
        "id": "hBVhk6adFg1j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Summary**  \n",
        "I was into the data representation part of rereading my past notes."
      ]
    }
  ]
}