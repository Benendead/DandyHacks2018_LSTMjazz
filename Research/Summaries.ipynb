{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summaries",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benendead/LSTMjazz/blob/master/Research/Summaries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "HtIUKuFX-QLO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Topics Left To Study** (Paper Mentioned In)  \n",
        "* RTRL error (Eck 2002)\n",
        "* BPTT (Eck 2002, Lackner 2016)\n",
        "* Kalman filter to update weights? (Eck 2002)\n",
        "* [RBM](http://imonad.com/rbm/restricted-boltzmann-machine/) (Bickerman 2010)\n",
        "* [Contrastive Divergence](http://www.cs.toronto.edu/~fritz/absps/tr00-004.pdf) (Bickerman 2010)\n",
        "* Fourier Transform (Lackner 2016, Hilscher 2018)\n",
        "* [Simulated annealing](https://cs.stackexchange.com/questions/79241/what-is-temperature-in-lstm-and-neural-networks-generally) (Hilscher 2018)\n",
        "* Autocorrelation w/ FFT (Hilscher 2018)\n",
        "* Seq2Seq (Agarwala 2017)\n",
        "* Bidirectionality, attention (Agarwala 2017)\n",
        "* Temperature parameters (Hilscher 2018, Brunner 2017)"
      ]
    },
    {
      "metadata": {
        "id": "5uGIymurBuLz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Summaries"
      ]
    },
    {
      "metadata": {
        "id": "qA1Ycmlt5jC5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Choi et al. (2016)"
      ]
    },
    {
      "metadata": {
        "id": "aS2l3cVl5lA4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Summary**  \n",
        "They used char-RNN and word-RNN to generate jazz chord progressions. Their data was 2,486 scores transposed to C from real books. The models were fed text data of chord progressions, with all quarter notes filled in, as in:  \n",
        "-START- F:9 F:9 F:9 F:9 D:min7 D:min7 G:9 G:9  \n",
        "C:maj C:maj F:9 F:9 C:maj C:maj C:maj C:maj -END-  \n",
        "There were 39 distinct characters, 1,259 distinct chords, and 539,609 chords overall."
      ]
    },
    {
      "metadata": {
        "id": "yowVS08H7rQc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Why char-RNN?**  \n",
        "Benefits of char-RNN:\n",
        "* Smaller vocabulary\n",
        "* Fewer assumptions about music made\n",
        "\n",
        "Downsides of char-RNN:\n",
        "* Shorter effective length of memory, as more time steps are needed to input the same information"
      ]
    },
    {
      "metadata": {
        "id": "fXX0MAOW64rf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Model Architecture**  \n",
        "Two LSTM layers with 512 hidden units (hidden state dimensionality) and then Dropout of 0.2 after each LSTM layer. Put together in Keras with categorical cross entropy as loss, Adam optimizer, and stochastic prediction based on a diversity parameter $\\alpha$. New probabilities are calculated:  \n",
        "$\\hat{p}_i=e^{log(p_i)/\\alpha}$, where $p_i$ is the probability for the $i$ states.  \n",
        "A state is then selected based on the probabilities."
      ]
    },
    {
      "metadata": {
        "id": "Bk1HWxDS8_1L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Successes**  \n",
        "Their char-RNN learned $\\text{ii}-\\text{V}-\\text{I}$, passing chords, modal interchange chords, and tritone substitutions. The word-RNN included modal interchange, circle of fifths progressions, and descending basslines. Their code is available [here](https://github.com/keunwoochoi/lstm_real_book).  \n",
        "*-Fin 1/24/2019-*"
      ]
    },
    {
      "metadata": {
        "id": "hbpRoMjp9yWc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Eck et al. (2002)"
      ]
    },
    {
      "metadata": {
        "id": "KVLPVy2b99J2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Summary**  \n",
        "This paper ran two experiments with LSTMs:\n",
        "1. Memorizing a chord progression.\n",
        "2. Also improvising pentatonic blues over a memorized progression.  \n",
        "\n",
        "Both experiments were successful using relatively simple LSTMs. They represented the data as a vector with 12 possible chord notes and 13 melody notes. Their quantization step was an eighth note and note representation was a simple 1/0 on/off input.\n",
        "\n",
        "Examples can be found [here](https://web.archive.org/web/20190104192500/http://people.idsia.ch/~juergen/blues/)."
      ]
    },
    {
      "metadata": {
        "id": "F0ByvqcJBxjz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Experiment 1**  \n",
        "The data was just the same progression repeatedly. The network architecture used 4 LSTM cells each with 2 hidden cells all fully connected to each other and the input layer. Forget, input, and output gate biases were set to -0.5, -1.0, -1.5, and -2.0 for the four blocks, respectively. Output biases were 0.5, learning rate 0.00001, momentum rate 0.9. Loss was cross-entropy and output function was logistic sigmoid.\n",
        "\n",
        "In the output, chord notes were on if they had a probability above 0.5. Training ended once the chord sequence was completely memorized."
      ]
    },
    {
      "metadata": {
        "id": "rc8TWaKoDOCf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Experiment 2**  \n",
        "Some cell blocks learned melody and others learned chords. There were eight cell blocks total with two hidden cells each. Melody information never reached chord cell blocks, although everything else was identical to Experiment 1."
      ]
    },
    {
      "metadata": {
        "id": "Q34CIwpqD0Zy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again, chord outputs had a 0.5 threshold. The melody output needed to pick one note, so the probabilities were constrained to sum to 1 and then a random number in [0,1] was used. Training ended until objective error plateaued. Outputs were seeded and then allowed to compose progressions.  \n",
        "*-Fin 10/24/2019-*"
      ]
    },
    {
      "metadata": {
        "id": "6YXHNtMTE3KI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Bickerman et al. (2010)"
      ]
    },
    {
      "metadata": {
        "id": "hBVhk6adFg1j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Summary**  \n",
        "This work used Deep Belief Networks (DBNs) to compose monophonic jazz music given some chord sequence. Their data was a corpus of 4-bar jazz licks cycling over the $\\text{ii}-\\text{V}-\\text{I}-\\text{VI}^7$ turnaround progression. They transposed these progressions up 0, 1, 2, and 3 semitones and the model was still able to produce chord-compatible melodies. Code is [here](https://sourceforge.net/projects/rbm-provisor/)."
      ]
    },
    {
      "metadata": {
        "id": "JyAWNceTs-BU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Representation**  \n",
        "They took the 4-bar examples and encoded them as bit vectors with 12 time steps (or \"slots\") per beat. Each time step had 30 bits split up as:\n",
        "* 18 melody bits with 12 indicating the note's pitch (1-hot), 4 indicating octave (1-hot), 1 for sustained note, and 1 for rests.\n",
        "* 12 chord bits with 1/0 indicating on/off for each possible note in a chord.\n",
        "\n",
        "These two parts were concatenated to form one time step's input."
      ]
    },
    {
      "metadata": {
        "id": "j-WqPNs7uStc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**DBNs**  \n",
        "The DBN is simply a multi-layer RBM. An input layer is set to the input, bi-directional weights are used to calculate the fully-connected hidden layer, and then the same weights are used to set the input layer. This repeats until stabilization. DBNs just extend this by adding more hidden layers, which allows them to learn features of learned features.\n",
        "\n",
        "Music was generated by \"clamping\" the chord bits down from an input, giving a random melody seed, and letting the trained network stabilize for the next few beats of melody. The input is shifted down one beat, the past beat of melody is clamped, and the process repeats. To select which notes to play, the melody uses only the highest probability of pitch and octave and chooses to play that, sustain, or rest.\n",
        "\n",
        "The authors found that a DBN with 4 layers total of 1441 input nodes (4 beats, 12 slots each with 30 bits/slot plus 1 bias bit) and 750, 375, and 200 hidden nodes. Training lasted 250 epochs over the 100 4-bar licks."
      ]
    },
    {
      "metadata": {
        "id": "ooyeALP4vRRo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Results**  \n",
        "The generated melodies were mostly chord tones with some color, while foreign notes were avoided. Large jumps, octaves, half-step approach tones, and triplets were all avoided by the DBN. Repeated notes were overly common in the generated music.  \n",
        "*-Fin 1/25/2019-*"
      ]
    },
    {
      "metadata": {
        "id": "q72t-IDZyprq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Lackner (2016)"
      ]
    },
    {
      "metadata": {
        "id": "RuqkAgiKzn6V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Summary**  \n",
        "This paper tested multiple LSTM architectures with the goal of composing monophonic melodies for given 8-bar chord sequences."
      ]
    },
    {
      "metadata": {
        "id": "HJw3FUx-srk6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Dataset and Representation**  \n",
        "The LSTMs were trained on 68 8-bar sections of Beatles music from the book \"Pop Classics For Piano: The Very Best Of The Beatles - Easy Arrangements for Piano.\" The chord notes ranged from C2-B2 and the melodies' from C3-B4. Quantization was to the 16th note.\n",
        "\n",
        "For the data representation, MIDI was used and segmented into a piano roll matrix. Notes were 1/0 on/off with the second half of any note indicated by a vector of all 0s. Thus the matrix was size $(2*(\\text{# timesteps}), \\text{note range})$. This matrix was sampled into $n$-timestep-long sequences of chord data. Each sample was given a label with the melody information for the $n+1$ timestep. The number of samples, $S$, was the total length of the data (in timesteps) minus $n+1$, as the penultimate time step needs the ultimate for a target vector.\n",
        "\n",
        "Thus the data was input into Keras as an input matrix of size $(S, n, |\\text{chord pitch range}|)$. The target matrix was then $(S, |\\text{melody pitch range}|)$. The number of samples was the total length (in timesteps) of the data minus $n$, the sequence length."
      ]
    },
    {
      "metadata": {
        "id": "K37LS-jQwauh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**LSTM Architecture**  \n",
        "The LSTM input layer took chord notes (thus 12 nodes), was fully connected to optional hidden layers, and then was fully connected to an output layer with 24 LSTM nodes corresponding to melody notes. The highest melody probability is chosen; if it's above some value, that note is played.\n",
        "\n",
        "To generate new music, any chord sequence can be input and the resulting output can be converted to MIDI. Many architectures were tested; the best had 2 hidden layers with 9 and 18 LSTM units, respectively."
      ]
    },
    {
      "metadata": {
        "id": "_nzDhCz8y3PW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Results**  \n",
        "The generated music was evaluated with human listeners:\n",
        "* LSTM melodies were preferred over those by humans in 27% of cases (including testing on chords the LSTM had never seen).\n",
        "* LSTM melodies were differentiable from human compositions in 60% of cases.\n",
        "\n",
        "Future work may want to pursue a larger training dataset size, more acute compositional goals and tailored data, and another data representation style.  \n",
        "*-Fin 1/25/2019-*\n"
      ]
    },
    {
      "metadata": {
        "id": "sBZgQ8XOzoyl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. Hilscher et al. (2018)"
      ]
    },
    {
      "metadata": {
        "id": "yKeaazyj0oU7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Summary**  \n",
        "This work implemented a char-RNN to generate polyphonic classical piano pieces. They used a MIDI dataset of classical music, specifically:  \n",
        "* 194 Bach pieces, 1.4M characters\n",
        "* 39 Mozart pieces, 0.4M characters\n",
        "* 335 tracks by multiple composers, pianomidide, filtered down to 132 tracks with 0.8M characters"
      ]
    },
    {
      "metadata": {
        "id": "WaEdzeGzM0LL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Representation**  \n",
        "They quantized the MIDI data to 16th notes, then assigned each pitch its own ASCII character. The data was augmented to normalize tempo as well as transposed up 0-5 semitones. The characters \"on\" for each time step were concatenated together and then separated from the next time step by a space. These characters were then 1/0 on/off encoded into vectors."
      ]
    },
    {
      "metadata": {
        "id": "l0FAfhraNbh6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Architecture**  \n",
        "Their char-RNN used a many-to-one architecture, with many input time steps were giving a single output time step. The final layer was a dense softmax which gave probabilities for each character. A temperature value was used to tune how diverse the output notes were.\n",
        "\n",
        "The best architecture used a single LSTM layer of 512 units, sequence length of 100, Mozart data fully normalized/transposed, batch shuffling, with categorical cross entropy loss, Adam optimizer, 0.001 learning rate, and validation split of 0.2."
      ]
    },
    {
      "metadata": {
        "id": "dGUGNQwPQfmy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Results/Future Work**  \n",
        "Wave signal analysis was done to show that the network was not simply reproducing the training data. Temperature experiments showed that the network had not seen enough data. Overall their model achieved 90% training and 70% validation accuracy. Their results can be seen [here](https://yellow-ray.de/~moritz/midi_rnn/examples.html).\n",
        "\n",
        "Future work may want to consider Seq2Seq, further augmentations to different keys, training on all data at once, other music notations, incorporating MIDI velocity (note dynamics), multiple instruments, or textual pattern matching to measure overfitting.  \n",
        "*-Fin 1/26/2019-*"
      ]
    },
    {
      "metadata": {
        "id": "rqM6cv7gSB74",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6. Agarwala et al. (2017)"
      ]
    },
    {
      "metadata": {
        "id": "o7uGFsoPTMG9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Summary**  \n",
        "This work tested multiple models' abilities to generate syntactically correct monophonic music. The algorithms tested were Continuous Bag-of-Words (CBOW), Char-RNN, Sequence-to-Sequence (Seq2Seq), and Generative Adversarial Networks (GANs). Of these, only Char-RNN and Seq2Seq produced noteworthy results. Code is [here](https://github.com/yinoue93/CS224N_proj)."
      ]
    },
    {
      "metadata": {
        "id": "zrcigvbsV5yJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Representation**  \n",
        "This work used ABC data, a discrete character-based notation which forces models to construct syntactically correct music. They collected 34,000 ABC examples from abcnotation.com and thesession.com. These were transposed to 4 random keys, encoded into integer representations, which were fed into the embedding layers of the successful models. The data was split 80/10/10 into train/test/dev sets."
      ]
    },
    {
      "metadata": {
        "id": "P-xoUKE4a3bx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Architecture Specifics**  \n",
        "Char-RNN: A hidden layer of 200 LSTM cells with 0.2 dropout and embedding size of 20 were chosen. Softmax was used to predict the next character. An input window of size 50 worked best.  \n",
        "Seq2Seq: Input size was set to 25. In the end of experimentation, a hidden state size 800, embedding size 100, unidirectional cells, and Bahdanau attention were chosen."
      ]
    },
    {
      "metadata": {
        "id": "jil3k3IVWp07",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Results**  \n",
        "The char-RNN was able to compose somewhat passable music, but it had issues with barline placement. The Seq2Seq model was not only able to correctly utilize barlines, but also replicate motifs from a warm start and even compose duets. The char-RNN achieved 59.5% validation accuracy, while the Seq2Seq managed 65%. The latter also fooled 70% of humans into thinking its music was human-composed.  \n",
        "*-Fin 1/27/2019-*"
      ]
    },
    {
      "metadata": {
        "id": "1h4cmgDOcMN9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7. Brunner et al. (2017)"
      ]
    },
    {
      "metadata": {
        "id": "wj0d1wXjdA1W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Summary**  \n",
        "This work implemented an LSTM-based model which generates polyphonic compositions with long-term structure. There are two steps:\n",
        "1. A chord LSTM generates a chord progression based on learned chord embeddings.\n",
        "2. A polyphonic LSTM generates music from this chord progression.\n",
        "\n",
        "Their approach generalized the circle of fifths from training data in its chord embedding."
      ]
    },
    {
      "metadata": {
        "id": "iwv5z4bwh3Ez",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Representation**  \n",
        "They used the Lakh MIDI Dataset, which has 114,988 MIDI songs. They analyzed songs' key by their 7 most common notes to find that 86,711 were in major or a relative minor. These were all shifted to C; only notes in the range $\\text{C2}-\\text{C6}$ were considered.\n",
        "\n",
        "They then extracted chords by computing the three most-used notes in each bar of each song. They gave the top 50 most-common chords unique integer IDs and the rest an \"unknown\" tag. These were then encoded as vectors and given unique real-number embeddings of size 10. Quantization was set to 8 time steps per measure."
      ]
    },
    {
      "metadata": {
        "id": "p7xBtMCbjtVG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Model Architecture**  \n",
        "Chord LSTM: This model learned those chord embeddings. From this layer, they used a hidden layer with 256 LSTM cells followed by a $\\text{softmax}$ activation. The output corresponded to a vector of probabilities for the next chord. Training used cross-entropy as loss, Adam optimizer, $10^{-5}$ initial learning rate, and 80,000 of the shifted songs for 4 epochs. To generate new progressions, they seed the model and then sample output probabilities with temperature. This is fed in and the cycle repeats.\n",
        "\n",
        "Polyphonic LSTM: This LSTM received vectors of piano rolls (1/0 on/off made with the pretty_midi library) with these appended features:\n",
        "1. Embedded chord vector of the next time step.\n",
        "2. Embedded chord vector of the chord following that chord.\n",
        "3. A binary counter from 0 to 7 each bar.\n",
        "\n",
        "The input is fed into an LSTM with 512 hidden cells and $\\text{sigmoid}$ activation. The output at each time step is the probabilities for each note being played. Training used cross entropy between outputs and the ground truth, Adam, initial learning rate of $10^{-6}$, and only 10,000 songs for 4 epochs.\n",
        "\n",
        "To generate a new song, the polyphonic LSTM is seeded with the piano roll and corresponding chords. The next step is sampled and the number of notes to play at any one time is limited. To deal with ambiguous note endings/re-attacks, they consider all consecutive same notes as held notes. At barlines, all played notes are re-attacked."
      ]
    },
    {
      "metadata": {
        "id": "eidJJAzUmE9u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Results and Future Work**  \n",
        "The chord LSTM learned embeddings that, when shifted with PCA, show the circle of fifths. The results contain chord progressions typical in western music. Temperature can be used to control this and the melodic variety.\n",
        "\n",
        "Future work may want to address:  \n",
        "* Data representations for held notes only exist for monophonic models.\n",
        "* Another level to their system's hierarchy might guide the construction of verse/bridge/chorus structures.\n",
        "* The Lakh MIDI Dataset includes artist and genre metadata. Perhaps a future system could use this genre feature.\n",
        "\n",
        "*-Fin 1/27/2019-*"
      ]
    }
  ]
}